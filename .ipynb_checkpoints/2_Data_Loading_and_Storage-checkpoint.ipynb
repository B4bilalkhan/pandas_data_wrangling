{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yQWraYXXCHo"
   },
   "source": [
    "# Loading and Storing Data \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In the previous chapter, `pandas` Data Structures, we interacted with the two primary data structures of the `pandas` package: `Series` and `DataFrames`. While manually building these `pandas` objects was a good learning experience, in practice this is not always an efficient or feasible workflow as real life datasets can consist of millions of observations. For this reason, data are commonly stored on computers in standardized formats. Thus the process of reading the data and putting it into your favorite data structure (such as `DataFrames`) can be automated. By the end of this chapter you will know how to transfer data between plain text files and `pandas` `DataFrames`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LpRtqgMbmqCR"
   },
   "source": [
    "## Preparing our Environment\n",
    "\n",
    "---\n",
    "\n",
    "Before we begin, we must prepare our environment by loading all the packages we will be working with. For this chapter we will only be needing the `pandas` toolkit. To import the package we will use the standard alias `pd`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVqiknhAmo9Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SWYudpNDfgQj"
   },
   "source": [
    "# About the Data\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this Chapter we will be using a subset of the publicly available data on medical spending from the Centers for Medicare & Medicaid Services website ([`CMS` website](https://www.cms.gov/OpenPayments/Explore-the-Data/Dataset-Downloads.html)). This data set describes medication orders made by individual doctors and contains the following columns:\n",
    "\n",
    "| Column |Description|\n",
    "|:----------|-----------|\n",
    "| `unique_id`| A unique identifier for a Medicare claim to CMS |\n",
    "| `doctor_id` | The Unique Identifier of the doctor who <br/> prescribed the medicine  |\n",
    "| `specialty` | The specialty of the doctor prescribed the medicine |\n",
    "| `medication` | The medication prescribed |\n",
    "| `nb_beneficiaries` | The number of beneficiaries the <br/> medicine was prescribed to  |\n",
    "| `spending` | The total cost of the medicine prescribed <br/>for the CMS |\n",
    "\n",
    "We will be working with the data stored in multiple files including `'Data/spending_csv_ex.csv'`, `'Data/spending_10k.csv'`,  'data/spending_10k_no_header.csv', 'Data/spending_missing_values.csv', 'Data/spending_comment.csv',  and 'Data/spending_footer.csv'. These file are constructed from the original data set to illustrate certain points more clearly and simulate issues you may run into when reading data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ez9gn4OJa_Cf"
   },
   "source": [
    "# Reading Data in Text Format\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z5DziNr-ffEy"
   },
   "source": [
    "## Common Plain Text File Formats\n",
    "\n",
    "The plain text file is one of the most common and simple ways to store data. Plain text files only contain text without any special formatting or code. A non-example would be a `.doc` word document since it contains associated code that formats the text, like the font, the margins for the page, the spacing and so forth. However, Word allows you to export a file as plain text with file extension `.txt`. There are many plain text editors available, some of the most popular include [atom](https://atom.io/) and [emacs](https://www.gnu.org/software/emacs/).\n",
    "\n",
    "\n",
    "To store data in a plain text file you need a standard way of distinguishing the individual data entries. For example, suppose a file contained the follwing text:\n",
    "\n",
    "```\n",
    ",column1,column2,column3,\n",
    "row1,a,b,c\n",
    "row2,d,e,f\n",
    "row3,g,h,i\n",
    "```\n",
    "\n",
    "A human would see the text above and may be able to discern the 3 columns and 3 rows and the individual data entries. A computer would have no idea how to parse this without any direction; to a computer the text above is just one string of characters. To help the computer parse the text, we could tell it that each data entry is separated by a column and each row is separate by a new line. This way of organizing data is called a plain text file format.\n",
    "\n",
    "Most of the plain text file formats fall into one of the following two categories: **Delimited** and **Fixed Width**\n",
    "\n",
    "* Delimitted files are organized such that columns and rows are seperated by a certain character called a ***delimiter***\n",
    "* Fixed width files are those where each entry in a column has a fixed number of characters\n",
    "\n",
    "We will focus on the delimited `.csv` file type. `csv` is an acronym which stands for '**C**omma **S**eparated **V**alues' and informs us that the column entries are delimited by commas and the rows are delimited by a new line. So, the example we discussed in this cell conforms to the `.csv` format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1VeODNTlyT5k"
   },
   "source": [
    "## pandas Parsing Functions\n",
    "\n",
    "Included in the pandas toolkit is a [collection](https://pandas.pydata.org/pandas-docs/stable/io.html) of parsing functions to read and build `DataFrames` from data that is stored in a variety of formats. The `pandas` function which parses and builds a `DataFrame` from a generic delimited plain text file is `read_csv()`. To run `read_csv()` with the default settings we only need to provide 1 positional argument, the path to the file we want to read. If we wanted to read the file whose relative path is $p$ and put the data into a new `pandas` `DataFrame`, `df`, we would type `df = pd.read_csv(p)`.  \n",
    "\n",
    "Parsing plain text files can become a complicated procedure. To aid in the process, `pandas` provides a lot of optional parameters that may be set when calling the `read_csv()` function. To learn more, see the `read_csv()` [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), which briefly summarizes all of the parameters. We will be covering many of the most important parameters as we work through chapter. \n",
    "\n",
    "By default `read_csv()` will seperate data entries when it sees commas and will seperate rows by new lines, which is encoded by: `'\\n'`. If we wanted to change this behavior so that `read_csv()` separates by tabs (encoded with `\\t`), then we can set the optional parameter `sep = '\\t'`.  For instance, if we wanted to read the data in the file 'spending_csv_ex.tsv', which is a tab separated values file, and save the data in a `pandas` `DataFrame` called `df`, then we would type:\n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('Data/spending_csv_ex.tsv', sep='\\t')\n",
    "```\n",
    "\n",
    "Though`read_csv()` can handle `.tsv` files, there is a specific parsing function for `.tsv`  files: `read_table()`. The difference between `read_table()` and `read_csv()` is that the default behavior for the latter is to separate using commas instead of tabs `‘\\t’`. The `read_table()` documentation is available [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_table.html) for more information.\n",
    "\n",
    "To perform the same operation (that is reading the data in the file 'spending_csv_ex.tsv' and save the data in a `pandas` `DataFrame` called `df`), we may use the `read_table()` function without having to define our delimiter, since the default parameters will correctly parse our file. \n",
    "\n",
    "```python\n",
    ">>> df = pd.read_table('Data/spending_csv_ex.tsv')\n",
    "```\n",
    "\n",
    "Both methods, using `read_table()` and `read_csv()`, result in the equivalent `DataFrame` seen below\n",
    "\n",
    "```python\n",
    ">>> print(df)\n",
    "      unique_id   doctor_id   specialty   medication  nb_beneficiaries    spending\n",
    "0  AB789982  1952310666  Psychiatry   CLONAZEPAM               226  \\$1,848.88\n",
    "1  AV967778  1952310666  Psychiatry     DIAZEPAM               103    \\$662.87\n",
    "2  CC128705  1298765423  Cardiology      NADOLOL                13         NaN\n",
    "3  GH890091  1346358827      Family  HYDROCODONE               331  \\$8,511.14\n",
    "4  YY219322  1548247315  Psychiatry   ALPRAZOLAM                28  \\$1,964.49\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNwdUeIQ5Umo"
   },
   "outputs": [],
   "source": [
    "df = pd.read_table('Data/spending_csv_ex.csv', \n",
    "                   sep=',')\n",
    "print(df)\n",
    "print('----------------------------------------------')\n",
    "df = pd.read_csv('Data/spending_csv_ex.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxhJqPx5hFU6"
   },
   "source": [
    "## Specifying the Path to Your Data File\n",
    "\n",
    "We saw in the previous section, that to use the pandas parsing functions, we needed to provide the function with not only the name of the file but its path as well, i.e. where the file is located. \n",
    "\n",
    "To specify the path to a file, we can either use 1) the **absolute path** of the file, or 2) the **relative path** starting from the current working directory (in the previous section we used the relative path).\n",
    "\n",
    "1. To use the **relative path** you may begin with directories that are in the current working directory of your environment and specify the path to the file from there.\n",
    "\n",
    "2. To find the **absolute path** of a file you can navigate to the directory of the data file using your terminal and use the `pwd` command if you are using a linux or linux like machine, or the prompt in the windows command line, to obtain the directory path that the file is in. The directory path and the file name concatentated creates the absolute path to the file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1lr-uXHaAZI"
   },
   "source": [
    "# Exercise 2.1: Reading Data From Plain Text Files\n",
    "\n",
    "Which line of code will correctly parse the data stored in the file `hawaii_toursim.txt` which is stored in the current working directory, if the data is formatted in the following way:\n",
    "\n",
    "```\n",
    "Year : Visitor_Arrival_Total : Visitor_Arrival_International\n",
    "1966 : 834732                : 205168\n",
    "1967 : 1124012               : 295163\n",
    "1968 : 1313706               : 360885\n",
    "```\n",
    "\n",
    "so that the `DataFrame`, `Hawaii_tourism_df` is structured as is shown in the table below?\n",
    "\n",
    "|- | Year |   Visitor Arrival Total    | Visitor Arrival International |\n",
    "|:--------------------------|:-------------------|:-------------------|:-----------------------|\n",
    "|   0  |   1966      |  834732 |    205168|\n",
    "|   1  |   1967      |  1124012 |  295163|\n",
    "|   2  |   1968      |  1313706 |  360885|\n",
    "\n",
    "A:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_table('hawaii_toursim.txt')\n",
    "```\n",
    "\n",
    "B:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_table('hawaii_toursim.txt', sep=':')\n",
    "```\n",
    "\n",
    "C:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_csv('hawaii_toursim.txt')\n",
    "```\n",
    "\n",
    "D:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_csv('hawaii_toursim.txt', sep=:)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViC460qzh0sc"
   },
   "source": [
    "## Partially Loading Data \n",
    "\n",
    "When working with large datasets it is good practice to load your data in small pieces before loading the entire dataset to ensure that the file is parsed correctly. Small data sets are more manageable and errors are easier to spot, while large data sets take more time to parse. So, a good workflow is to read a small portion of the data and analyze the resulting data frame to see if you need to modify any of the default behaviors of the read function.\n",
    "\n",
    "To load only up to a limitted number of rows we can use the `nrows` parameter for both `read_table()` and `read_csv()`. For example, the file `spending_10k.csv` is a `csv` file with 10,000 rows, but if we wanted to read only the first 5 rows of this file we can call the `pandas` `read_csv()` function and set `nrows = 5`:\n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('Data/spending_10k.csv', nrows=5)\n",
    ">>> df.shape\n",
    "(5,6)\n",
    "```\n",
    "\n",
    "We see that the `DataFrame` `df` that we saved the data in, has a shape attribute of `df.shape=(5,6)`. This means that there are  5 rows (since we set `nrows=5`) and 6 columns (all the columns of the dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOIySWAYaVMt"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/spending_10k.csv', nrows=5)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wLVTFwrctm8"
   },
   "source": [
    "## Indexing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wj5WypA5d07T"
   },
   "source": [
    "### Headers\n",
    "\n",
    "Some `.csv` files will have **headers**. **Headers** are the first row of the table and contain the column labels.\n",
    "\n",
    "![](images/header.png)\n",
    "\n",
    "By default `read_csv()` and `read_table()` will assume the file has a header and use the first row of the table to label the columns, but this is not always desired behavior. If a file doesn't have a header and we keep the default behavior of `read_csv()` and `read_table()`, then the column labels will be the first row of data entries. For instance, 'spending_10k_no_header.csv' is a file with 10,000 rows and no header labeling the columns. If we read the first 4 rows of this file, by setting `nrows=5`, then we see the following results:\n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('Data/spending_10k_no_header.csv', nrows=4)\n",
    ">>> df.columns\n",
    "Index([u'YS123432', u'1841589744', u'CARDIOLOGY', u'VENLAFAXINE HCL', u'25',\n",
    "       u'265.49'],\n",
    "      dtype='object')\n",
    "```\n",
    "\n",
    "The columns attribute of the `DataFrame`, which we access using `df.columns`,  are not labels. \n",
    "\n",
    "To change how the parsing functions `read_table()` and `read_csv()` assign the column labels of the new `DataFrame` we set the optional `header` parameter in the function call. If the file has no header, i.e. the first row has data, then we can set the optional parameter to `header = None`. \n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('Data/spending_10k_no_header.csv', nrows=4, header=None)\n",
    ">>> df \n",
    "          0           1           2                     3    4        5\n",
    "0  YS123432  1841589744  CARDIOLOGY       VENLAFAXINE HCL   25   265.49\n",
    "1  CG916968  1952344418  CARDIOLOGY           SIMVASTATIN   85   767.83\n",
    "2  MK361461  1245206184  CARDIOLOGY   PANTOPRAZOLE SODIUM  102  1608.42\n",
    "3  CG865025  1497955603  CARDIOLOGY  ATORVASTATIN CALCIUM   82  2726.72\n",
    "```\n",
    "\n",
    "We see in the example above that when we set `header=None`, the columns of the `DataFrame` are set to the default range of integers.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_Rqxm9D3Lb-"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/spending_10k_no_header.csv', nrows=4)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXMW-fUC3Qmw"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/spending_10k_no_header.csv', nrows=4, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyGShZh-eUAM"
   },
   "source": [
    "### Index Columns\n",
    "\n",
    "You may have noticed that the default row index of `read_table()` and `read_csv()` is the range of integer values $[0,1,... N-1]$ where $N$ is the number of rows in the `DataFrame`. It is possible though to use an existing column in the data set to label the rows by setting the `index_col` parameter when calling `read_csv()` and `read_table()`. If the file has a header, then we can use the label of the column declared by the header to specify which column we want to use as the index. For example, if wanted to read the first 4 rows of the file 'spending_10k.csv' and set the index to be the column labeld as 'unique_id' in the header row of the data set, then we would type:\n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('data/spending_10k.csv', nrows=5, index_col='unique_id')\n",
    "```\n",
    "\n",
    "If the file doesn't have a header, then we may still set an index column, but instead we must use the integer location of the column. Remember the integer location is read from left to right starting from 0. So if we wanted to read the first 4 rows of the data in the file 'data/spending_10k_no_header.csv', which is a file with no header, and set the $0^{th}$ column as the index, then we would type:\n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('data/spending_10k_no_header.csv', nrows=5,  header=None, index_col=0)\n",
    "```\n",
    "\n",
    "In the example above, the index object will not initially have a name attribute since there was no header in the file specifying the name of the column we used, but we may give a name to the index after the `read_csv()` call by accessing the `name` attribute of  `df's index`. If we wanted to set the name of the index to 'unique_id' then we would type:\n",
    "\n",
    "```python\n",
    ">>> df.index.name = 'unique_id'\n",
    "```\n",
    "\n",
    "Both methods (assigning an index column by label and by position then setting the name attribute) result in the same `Index` object for the `DataFrame` `df`\n",
    "\n",
    "```python\n",
    ">>> df.index\n",
    "Index(['YS123432', 'CG916968', 'MK361461', 'CG865025', 'FV632964'], dtype='object', name='unique_id')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9OuPwZzdjkX"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/spending_10k.csv', nrows=5, index_col='unique_id')\n",
    "print(df.index)\n",
    "print('---------------')\n",
    "df = pd.read_csv('Data/spending_10k_no_header.csv', nrows=5,  header=None, index_col=0)\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M89ZjvC3pEZ2"
   },
   "source": [
    "# Exercise 2.2: Indexing with `pandas` Parsing Functions\n",
    "\n",
    "Which line of code will correctly parse the data stored in the file `hawaii_toursim.txt` which is stored in the current working directory, if the data is formatted in the following way:\n",
    "\n",
    "```\n",
    "1966, 834732, 205168\n",
    "1967, 1124012, 295163\n",
    "1968, 1313706, 360885\n",
    "```\n",
    "\n",
    "so that the `DataFrame`, `Hawaii_tourism_df` is structured as is shown in the table below?\n",
    "\n",
    "| | 0 |   1    |\n",
    "|:--------------------------|:-------------------|:-------------------|\n",
    "|  1966      |  834732 |    205168|\n",
    "|  1967      |  1124012 |  295163|\n",
    "|  1968      |  1313706 |  360885|\n",
    "\n",
    "A:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_table('hawaii_toursim.txt')\n",
    "```\n",
    "\n",
    "B:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_table('hawaii_toursim.txt', sep=',', header=None)\n",
    "```\n",
    "\n",
    "C:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_csv('hawaii_toursim.txt', header=None, index_col=0)\n",
    "```\n",
    "\n",
    "D:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_csv('hawaii_toursim.txt', index_col=0)\n",
    "```\n",
    "\n",
    "*Hint: Notice that the first column of the data set in the file is used as the index of the `DataFrame`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_N9RwW0cUuw"
   },
   "source": [
    "## (Some) Unclean Data Issues\n",
    "\n",
    "Though data storage in plain text files should follow certain formats like `.csv` for '**C**omma **S**eparated **V**alues' and `.tsv` for '**T**ab **S**eparated **V**alues', there is still some ambiguity on how things like missing entries, and comments should be denoted. For this reason `pandas` has implemented functionality into the `read_table()` and `read_csv()` to help 'clean' plain text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhc0yaD6drUF"
   },
   "source": [
    "### Missing Values\n",
    "\n",
    "There are often missing values in a real-world data set. These missing entries may be identifiable in the dataset by a number of different tags, like 'NA', 'N.A.', 'na', 'missing', etc. It is important to properly identify missing values when creating a `DataFrame` since certain `DataFrame` methods rely on the missing values being accounted for. For example, the `count()` method of a `DataFrame` returns the number of non missing values in each column. If we want this to be an accurate count, then we need to be sure of pointing out all the tags which represent 'missing' to `pandas`.\n",
    "\n",
    "Let us take a look at an example that will show us how we might spot these tags and let `pandas` know that they represent missing values. First, we will read the first 5 rows of the data in the file 'Data/spending_missing_values.csv',  and let’s assume that we want the index to be the column labeled by the header as 'unique_id'. \n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('Data/spending_missing_values.csv', index_col='unique_id', nrows=5)\n",
    ">>> df.loc[['TR390895', 'JA436080'], :]\n",
    "            doctor_id specialty          medication nb_beneficiaries spending\n",
    "unique_id                                                                    \n",
    "TR390895   1639597115      Null  LOSARTAN POTASSIUM             11.0    65.62\n",
    "JA436080   1073781571      Null         LAMOTRIGINE             12.0   8873.7\n",
    "```\n",
    "\n",
    "After inspection, we have found some rows with entries labeled as 'Null'. The example above explicitly prints these rows out using the `loc` attribute of the `DataFrame`. It is perfectly reasonable to want to recognize the 'Null' entries as missing.\n",
    "\n",
    "By default `read_table` and `read_csv` do not make assumptions on what represents missing values. To let `pandas` know that we want certain tags to be identified and missing, we can set the optional `na_values` parameter of the `read_table()` and `read_csv()` function. \n",
    "\n",
    "In our example we want to identfy 'Null' as missing, so we can set the optional parameter `na_values='Null'` when we call the `read_csv()` function.\n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('Data/spending_missing_values.csv', index_col='unique_id', nrows=5, na_values='Null')\n",
    ">>> df.loc[['TR390895', 'JA436080']]\n",
    "             doctor_id specialty          medication nb_beneficiaries spending\n",
    "unique_id                                                                    \n",
    "TR390895   1639597115      NaN  LOSARTAN POTASSIUM             11.0    65.62\n",
    "JA436080   1073781571      NaN         LAMOTRIGINE             12.0   8873.7\n",
    "```\n",
    "\n",
    "In the example above, we load the data into the `DataFrame` `df` and print the same two rows that we saw earlier had the 'Null' string entries. This time though the 'Null' string entries are replace with `NaN` entries. `NaN` represents missing values in `pandas` and will be handled properly when we call `DataFrame` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KxHYGL-V3rhj"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/spending_missing_values.csv', \n",
    "                 index_col='unique_id', nrows=5)\n",
    "df.loc[['TR390895', 'JA436080']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoJJHS7y3v_I"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/spending_missing_values.csv', \n",
    "                 index_col='unique_id', nrows=5, na_values='Null')\n",
    "df.loc[['TR390895', 'JA436080']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tu-RkAMUdIPA"
   },
   "source": [
    "### Skipping Rows\n",
    "\n",
    "There might be entire Rows or Columns that you will want to skip if they are not data or if they are data that is known to be corrupted. For example, let us look at a file that has some rows that are used as comments, 'Data/spending_comment.csv'. First we will read the first 5 rows of the file 'Data/spending_comment.csv' and assume that we know there is a header and we want the index to be the column labeled by the header as 'unique_id'. \n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('Data/spending_comment.csv', index_col='unique_id', nrows=5)\n",
    ">>> df \n",
    "                                    doctor_id   specialty  ...\n",
    "unique_id                                                                      \n",
    "# This dataset was made publicly available on t...          NaN                      NaN   \n",
    "# for Medicare & Medicaid Services website                   NaN                      NaN   \n",
    "AB789982                         1.952311e+09  Psychiatry   \n",
    "AV967778                         1.952311e+09  Psychiatry  \n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "\n",
    "We see the first two rows after the header are comments that we do not want in our `DataFrame`. `read_table()` and `read_csv()` have the optional `skiprows` parameter we may use to fix this. When `skiprows` is set equal to the list $n=[n_0, n_1, ..., n_k]$, `skiprows=n`, then  `read_table()` and `read_csv()` will skip the rows in the list. \n",
    "\n",
    "In our example we want to skip the rows 1 and 2. To do this we can set the skiprows parameter equal to the list $[1,2]$. \n",
    "\n",
    "```pyhton\n",
    ">>> df = pd.read_csv('Data/spending_comment.csv', index_col='unique_id', nrows=3, skiprows=[1,2])\n",
    ">>> df\n",
    "            doctor_id   specialty  medication  nb_beneficiaries   spending\n",
    "unique_id                                                                 \n",
    "AB789982   1952310666  Psychiatry  CLONAZEPAM               226  $1,848.88\n",
    "AV967778   1952310666  Psychiatry    DIAZEPAM               103    $662.87\n",
    "CC128705   1298765423  Cardiology     NADOLOL                13        NaN\n",
    "```\n",
    "\n",
    "We see that only the rows that were comments are no longer loaded into the `DataFrame` made from the `read_csv()` function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMeNsnHE4qIa"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/spending_comment.csv', \n",
    "                 index_col='unique_id', nrows=5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWuhq1cP4vxv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/spending_comment.csv', \n",
    "                 index_col='unique_id', nrows=3, skiprows=[1,2])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TI_zoC93put"
   },
   "source": [
    "### Skipping Columns \n",
    "\n",
    "We may only want a subset of the columns that are available from a data set stored in a plain text file. To do this we can use the `usecols` parameter of the `read_csv() `and `read_table()` functions. `usecols` specifies the column’s labels so that only columns with these labels will be loaded to the `DataFrame`.\n",
    "\n",
    "For instance, if we want to only load the columns labeled as 'specialty', 'spending', and 'unique_id' from the file 'Data/spending_comment.csv', which is the file with the comments in the rows 1, and 2 that we saw in the previous cell on Skipping Rows, then we may set the parameter `usecols=['specialty', 'spending', 'unique_id']` the following way: \n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('Data/spending_comment.csv', index_col='unique_id', nrows=3, skiprows=[1,2], usecols=['specialty', 'spending', 'unique_id'])\n",
    ">>> df \n",
    "            specialty    spending\n",
    "unique_id                       \n",
    "AB789982   Psychiatry  \\$1,848.88\n",
    "AV967778   Psychiatry    \\$662.87\n",
    "CC128705   Cardiology        NaN\n",
    "```\n",
    "\n",
    "The code above shows that we read the first 3 rows and columns labeld as 'specialty', 'spending', and 'unique_id' of the file 'Data/spending_comment.csv',  with the index set to be the column labeled by the header as 'unique_id', and skipping the rows 1 and 2. The resulting data frame is $3 \\times 2$ since the 'unique_id' column is used as the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mJ05WP04ziG"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/spending_comment.csv', \n",
    "                 index_col='unique_id', nrows=3, skiprows=[1,2], \n",
    "                 usecols=['specialty', 'spending', 'unique_id'])\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iH_hM6deSU34"
   },
   "source": [
    "### Skipping Footers\n",
    "\n",
    "A somewhat common practice is to include a **footer** at the end of a dataset in a plain text file with some comments. We can easily exclude the footer from our dataset using the `skipfooter` parameter. \n",
    "\n",
    "Let us look at what a footer would look like in a `DataFrame`. We first will read the data stored in 'Data/spending_footer.csv' using only the columns 'specialty', 'spending', and 'unique_id'. \n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('Data/spending_footer.csv', index_col='unique_id', usecols=['specialty', 'spending', 'unique_id'])\n",
    ">>> df \n",
    "                              specialty    spending\n",
    "unique_id                                                                \n",
    "AB789982                  Psychiatry  \\$1,848.88\n",
    "AV967778                  Psychiatry    \\$662.87\n",
    "CC128705                  Cardiology         NaN\n",
    "GH890091                      Family  \\$8,511.14\n",
    "YY219322                  Psychiatry  \\$1,964.49\n",
    "\\# This dataset...        NaN                NaN\n",
    "\\# for Medicare...        NaN                NaN\n",
    "```\n",
    "\n",
    "We see that there is an unwanted footer in the last two rows of the `DataFrame`. To fix this we can set the `skipfooter` parameter to 2, this will skip the last 2 rows of the `DataFrame`. \n",
    "\n",
    "```python\n",
    ">>> df = pd.read_csv('Data/spending_footer.csv', index_col='unique_id', usecols=['specialty', 'spending', 'unique_id'], skipfooter=2)\n",
    ">>> df \n",
    "                           specialty    spending\n",
    "unique_id                                                                \n",
    "AB789982                  Psychiatry  \\$1,848.88\n",
    "AV967778                  Psychiatry    \\$662.87\n",
    "CC128705                  Cardiology         NaN\n",
    "GH890091                      Family  \\$8,511.14\n",
    "YY219322                  Psychiatry  \\$1,964.49\n",
    "```\n",
    "\n",
    "The resulting `DataFrame` no longer has the footer at the last 2 rows of the `DataFrame` as was desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOEb2oNf48-j"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/spending_footer.csv', \n",
    "                 index_col='unique_id', \n",
    "                 usecols=['specialty', 'spending', 'unique_id'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0iwOSATS5ABH"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/spending_footer.csv', \n",
    "                 index_col='unique_id', \n",
    "                 usecols=['specialty', 'spending', 'unique_id'], \n",
    "                 skipfooter=2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecNOWcxg5T_j"
   },
   "source": [
    "# Exercise 2.3: Utilizing the Functionality of `pandas` Parsing Functions\n",
    "\n",
    "Which line of code will correctly parse the data stored in the file `hawaii_toursim.txt` which is stored in the current working directory, if the data is formatted in the following way:\n",
    "\n",
    "```\n",
    "# This dataset describes Hawaii's toursim industry\n",
    "Year, Visitor_Arrival_Total, Visitor_Arrival_International\n",
    "1966, 834732, 205168\n",
    "1967, 1124012,?\n",
    "1968, 1313706, 360885\n",
    "```\n",
    "\n",
    "so that the `DataFrame`, `Hawaii_tourism_df` is structured as is shown in the table below?\n",
    "\n",
    "| | 0 |   1    |\n",
    "|:--------------------------|:-------------------|:-------------------|\n",
    "|  1966      |  834732 |    205168|\n",
    "|  1967      |  1124012 |  NaN|\n",
    "|  1968      |  1313706 |  360885|\n",
    "\n",
    "A:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_csv('hawaii_toursim.txt', index_col=0, header=None, na_values='?', sep=',')\n",
    "```\n",
    "\n",
    "B:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_table('hawaii_toursim.txt', index_col='Year', skiprows=[0], na_values='?')\n",
    "```\n",
    "\n",
    "C:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_csv('hawaii_toursim.txt', na_values='?')\n",
    "```\n",
    "\n",
    "D:\n",
    "```python\n",
    "Hawaii_tourism_df = pd.read_csv('hawaii_toursim.txt',  index_col='Year', skiprows=[0], na_values='?')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQVvr52TcGxv"
   },
   "source": [
    " # Writing Data in Text Format\n",
    "\n",
    "---\n",
    "Now that we are familiar with the reading mechanisms that `pandas` has implemented for us, writing `DataFrames` to text files follows naturally. `pandas` `DataFrames` have a collection of `to_<filetype> `methods we can call; we will focus on `to_csv()`. `to_csv()` takes the parameter `path` (the name and location of the file you are writing) and will either create a new file or overwrite the existing file with the same name.\n",
    "\n",
    "```python\n",
    ">>> df.to_csv('Data/new_spending_data.csv')\n",
    "```\n",
    "\n",
    "`to_csv()` has a number of optional parameters that you may find useful, all of which can be found in the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CFD2cY850G1"
   },
   "outputs": [],
   "source": [
    "df.to_csv('Data/new_spending_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YROQ1DIxRcGv"
   },
   "source": [
    "# Summary\n",
    "\n",
    "---\n",
    "\n",
    "Some of the parameters of `read_csv` and `read_table` can be seen below:\n",
    "\n",
    "![](images/read_csv_Summary.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Data_Loading_and_Storage.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
