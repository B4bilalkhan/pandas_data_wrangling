{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZwsIw7ZQnr-"
   },
   "source": [
    "# Preparing Our Environment\n",
    "\n",
    "---\n",
    "\n",
    "`pandas` has a lot of powerful and easy to use functionality to manipulate and clean data, therefore, we will import the pandas toolkit with the standard alias `pd` for this chapter\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "We will also be making some basic plots for some examples to spot flaws in our data set, as we learned to do in the last chapter. To do this we use the python package `matplotlib.pyplot` under the standard alias `plt`.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "Lastly, some examples we will be working through require constructing `DataFrames` that are complex, and for convienence I will be using the `numpy` package with the standard alias `np`. You do not need to worry about the details of this package to understand this chapter, but, if you are interested, you can read the documentation for this widely used package here: [`numpy`](https://docs.scipy.org/doc/numpy/).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCz5hW3A4OoZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqcadNZH4qBy"
   },
   "source": [
    "# About the Data\n",
    "\n",
    "---\n",
    "\n",
    "In the last chapter we explored our data and saw some methods for spotting flaws, now we will discuss how to correct these errors. We will be using the same subset from the publicly available dataset from the Center for Medicare & Medicaid Services website ([`CMS` website](https://www.cms.gov/OpenPayments/Explore-the-Data/Dataset-Downloads.html)), and as a reminder our subset of data contains the following columns:\n",
    "\n",
    "| Column |Description|\n",
    "|:----------|-----------|\n",
    "| `unique_id`| A unique identifier for a Medicare claim to CMS |\n",
    "| `doctor_id` | The Unique Identifier of the doctor who <br/> prescribed the medicine  |\n",
    "| `specialty` | The specialty of the doctor prescribed the medicine |\n",
    "| `medication` | The medication prescribed |\n",
    "| `nb_beneficiaries` | The number of beneficiaries the <br/> medicine was prescribed to  |\n",
    "| `spending` | The total cost of the medicine prescribed <br/>for the CMS |\n",
    "\n",
    "We will specifically be looking at datasets that are intentionally constructed with some common issues so we can practice the data cleaning and preperation skills we will be covering in this chapter. The file that we will be using through this chapter is named 'spending_clean_ex.csv' and, relative to our current working directory, this file is in the folder 'Data'. We will be reading this .`csv` file and saving its contents into the `DataFrame` named `spending_df`. We also know ahead of time that this file contains the column `unique_id` which we will want to use to index our `DataFrame`. To do this we type:\n",
    "\n",
    "```python\n",
    "spending_df = pd.read_csv('Data/spending_clean_ex.csv', index_col='unique_id')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcmiQ3Ro-rdO"
   },
   "outputs": [],
   "source": [
    "spending_df = pd.read_csv('Data/spending_clean_ex.csv', index_col='unique_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mvT9LnMRA7fW"
   },
   "source": [
    "# Exercise 5.0: Importing the Honolulu Flights Data Set\n",
    "\n",
    "For some of the exercises in this chapter we will again be working with a data set containing information about all the arriving and departing flights in and out of the Honolulu aiport, HNL, on the Island of Oahu in December 2015. This data set was introduced in chapter 3: Exploring Data \n",
    "\n",
    "Please run the following code cell which will parse the 'honolulu_flights.csv' file, and build the `HNL_flights_df DataFrame` before trying the exercises in this chapter related to the Honolulu flights data set.\n",
    "\n",
    "Pleases recall that this data set contains the following columns:\n",
    "\n",
    "| Column |Description|\n",
    "|:----------|-----------|\n",
    "| `YEAR` | The year of the flight  |\n",
    "| `MONTH` |  The month of the flight |\n",
    "| `DAY` |  The day of the flight |\n",
    "| `DAY_OF_WEEK` |  The day of the week of the flight |\n",
    "| `FLIGHT_NUMBER` |  The flight number of the flight |\n",
    "| `ORIGIN_AIRPORT` |  The origin airport of the flight  |\n",
    "| `DESTINATION_AIRPORT` |  The destination airport of the flight |\n",
    "| `DEPARTURE_DELAY` |  The departure delay of the flight  |\n",
    "| `DISTANCE` |  The distance of the flight in miles |\n",
    "| `AIR_TIME` |  The flight time without taxiing in minutes |\n",
    "| `ARRIVAL_DELAY` |  The arrival delay of the flight  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UHXk8RoBT4F"
   },
   "outputs": [],
   "source": [
    "HNL_flights_df = pd.read_csv('Data/honolulu_flights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTG4gMfAQIr8"
   },
   "source": [
    "# Inspecting and Modifying Data Types\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In the last chapter we saw that `dtypes` was a `DataFrame` attribute which told us the `pandas` data type of each column. We noted that this was important since some methods only made sense to call on certain data types. Let us again inspect the data types of `spending_df`.\n",
    "\n",
    "```python\n",
    "spending_df.dtypes\n",
    "doctor_id             int64\n",
    "specialty            object\n",
    "medication           object\n",
    "nb_beneficiaries    float64\n",
    "spending             object\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "Please note two issues in this output, first, `doctor_id` is saved as an `int64`, and second, `spending` is saved as an `object`. These are issues since it wouldn't make sense to handle `doctor_id`s as numeric types (it wouldn't make sense to sum `doctor_id`s for example), also the `pandas object` data type is handled like a `Python` string, but it would be useful to perform numeric operations on the `spending` entries (it totally would make sense to sum `spending` for example).\n",
    "\n",
    "To solve this issue we need to cast the entries of the columns to a new `pandas` data type that is more appropriate. The method for this job is the `Series` `astype()` method. The `Series` `astype()` method will make a copy of the calling series and cast the new `Series` to the data type specified by the one positional argument, `dtype`.\n",
    "\n",
    "For instance, we are going to want to cast the `doctor_id` entries as `pandas objects` and make this cast permanent. To do this we can call the `Series` `astype()` method using the `spending_df` `doctor_id` column (which remember is a `Series`) and save the result into the same column, `doctor_id`.\n",
    "\n",
    "```python\n",
    ">>> spending_df.loc[:, 'doctor_id'] = spending_df.loc[:, 'doctor_id'].astype('object')\n",
    ">>> spending_df.dtypes['doctor_id']\n",
    "dtype('O')\n",
    "```\n",
    "\n",
    "Note that `dtype('O')` indicates that the datatyp is `pandas` *O*bject. This worked great, and was very easy, the  `doctor_id`s are now `pandas objects`.\n",
    "\n",
    "However, we cannot convert the `spending` column to `float64` using the same method, if we try using the same methodology we see the following results.\n",
    "\n",
    "```python\n",
    "spending_df.loc[:, 'spending'] = spending_df.loc[:, 'spending'].astype('float64')\n",
    "...\n",
    "ValueError: could not convert string to float: '$248.95'\n",
    "```\n",
    "\n",
    "The values contained in the `Series` `spending` are not compatible with the `float64` type, this is because a `float64` cannot contain \"`$`\" or \",\", therefore we will have to remove it from each entry. To change spending from `object` to `float64` will require `Series` string methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RDtHtqgC4ZXE"
   },
   "outputs": [],
   "source": [
    "spending_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3FneQJoNcPO"
   },
   "outputs": [],
   "source": [
    "spending_df.loc[:, 'doctor_id'] = spending_df.loc[:, 'doctor_id'].astype('object')\n",
    "print(spending_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzIL_Js7-lHW"
   },
   "source": [
    "# Exercise 5.1: Inspecting and Modifying Data Types\n",
    "\n",
    " Which table correctly states the data type of the columns of `HNL_flights_df` in its currect state (as it was imported in exercise 4.0)?\n",
    "\n",
    "A: \n",
    "\n",
    "| Column |Data Type|\n",
    "|:----------|-----------|\n",
    "| `YEAR` |  int64 |\n",
    "| `MONTH` |  int64 |\n",
    "| `DAY` |   int64 |\n",
    "| `DAY_OF_WEEK` |  int64 |\n",
    "| `FLIGHT_NUMBER` |  int64 |\n",
    "| `ORIGIN_AIRPORT` |  object  |\n",
    "| `DESTINATION_AIRPORT` | object  |\n",
    "| `DEPARTURE_DELAY` | float64  |\n",
    "| `AIR_TIME` | float64 |\n",
    "| `DISTANCE` | int64  |\n",
    "| `ARRIVAL_DELAY` | float64  |\n",
    "\n",
    "B: \n",
    "\n",
    "| Column |Data Type|\n",
    "|:----------|-----------|\n",
    "| `YEAR` |  object |\n",
    "| `MONTH` |  object |\n",
    "| `DAY` |   object |\n",
    "| `DAY_OF_WEEK` |  object |\n",
    "| `FLIGHT_NUMBER` |  int64 |\n",
    "| `ORIGIN_AIRPORT` |  object  |\n",
    "| `DESTINATION_AIRPORT` | object  |\n",
    "| `DEPARTURE_DELAY` | float64  |\n",
    "| `AIR_TIME` | float64 |\n",
    "| `DISTANCE` | int64  |\n",
    "| `ARRIVAL_DELAY` | float64  |\n",
    "\n",
    "C:  \n",
    "\n",
    "| Column |Data Type|\n",
    "|:----------|-----------|\n",
    "| `YEAR` |  object |\n",
    "| `MONTH` |  object |\n",
    "| `DAY` |   object |\n",
    "| `DAY_OF_WEEK` |  object |\n",
    "| `FLIGHT_NUMBER` |  object |\n",
    "| `ORIGIN_AIRPORT` |  object  |\n",
    "| `DESTINATION_AIRPORT` | object  |\n",
    "| `DEPARTURE_DELAY` | float64  |\n",
    "| `AIR_TIME` | float64 |\n",
    "| `DISTANCE` | int64  |\n",
    "| `ARRIVAL_DELAY` | float64  |\n",
    "\n",
    "\n",
    "*Hint: Feel free to use the code cell added below to explore the data types of `HNL_flights_df`*\n",
    "\n",
    "---\n",
    "\n",
    "Which line(s) of code will modify the data types of `HNL_flights_df`, inplace, so that it matches the following table?\n",
    "\n",
    "| Column |Data Type|\n",
    "|:----------|-----------|\n",
    "| `YEAR` |  object |\n",
    "| `MONTH` |  object |\n",
    "| `DAY` |   object |\n",
    "| `DAY_OF_WEEK` |  object |\n",
    "| `FLIGHT_NUMBER` |  object |\n",
    "| `ORIGIN_AIRPORT` |  object  |\n",
    "| `DESTINATION_AIRPORT` | object  |\n",
    "| `DEPARTURE_DELAY` | float64  |\n",
    "| `AIR_TIME` | float64 |\n",
    "| `DISTANCE` | int64  |\n",
    "| `ARRIVAL_DELAY` | float64  |\n",
    "\n",
    "A: \n",
    "The data types already match the table shown.\n",
    "\n",
    "B:\n",
    "```python\n",
    "HNL_flights_df.loc[:, 'YEAR'] = HNL_flights_df.loc[:, 'YEAR'].astype('object')\n",
    "HNL_flights_df.loc[:, 'MONTH'] = HNL_flights_df.loc[:, 'MONTH'].astype('object')\n",
    "HNL_flights_df.loc[:, 'DAY'] = HNL_flights_df.loc[:, 'DAY'].astype('object')\n",
    "HNL_flights_df.loc[:, 'DAY_OF_WEEK'] = HNL_flights_df.loc[:, 'DAY_OF_WEEK'].astype('object')\n",
    "HNL_flights_df.loc[:, 'FLIGHT_NUMBER'] = HNL_flights_df.loc[:, 'FLIGHT_NUMBER'].astype('object')\n",
    "```\n",
    "\n",
    "C:\n",
    "```python\n",
    "HNL_flights_df.loc[:, ['YEAR', 'MONTH',  'DAY', 'DAY_OF_WEEK', 'FLIGHT_NUMBER']].astype('object')\n",
    "```\n",
    "\n",
    "D. \n",
    "```python\n",
    "HNL_flights_df.astype('object')\n",
    "```\n",
    "\n",
    "*Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBWeB4SnIAqN"
   },
   "outputs": [],
   "source": [
    "# Exercise 4.1 Scratch code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bfQbaD9BQW2Z"
   },
   "source": [
    "# Series String Methods\n",
    "\n",
    "---\n",
    "\n",
    "`Series` contains various `string` processing methods that can be accessed using a `Series`’s `str` attribute. There are too many string methods to cover entirley, but we will cover perhaps the most important method, and that is `str.replace()`. You can see all of the `Series str` methods and descriptions [here](https://pandas-docs.github.io/pandas-docs-travis/api.html#string-handling)\n",
    "\n",
    "The `Series str.replace()`method  takes two required positional parameters; the first is the pattern we want to replace, and the second is the value to replace it with. Please not that `str.replace()` does not operate in place, i.e. it makes a copy of the calling `Series` and then the desired changes are made. Thus, if we wanted to create a new `Series`, $s_{0}$ from the `Series` of `pandas objects,` $s_{1}$, where each string in $s_{1}$ is modified so that every substring of the entries matching the pattern $p$ is replaced with the pattern $l$ we would type: `s_1 = s_0.str.replace(p, l)`.\n",
    "\n",
    "In our case we want to remove every dollar sign, '$\\$$' character. That seems different than what `str.replace()` does, but notice that removing  \"$\\$$\"  is equivalent to replacing it with the empty string, `\"\"`, similarly, removing \",\"  is equivalent to replacing it with the empty string, `\"\"`, therefore `str.replace()` is going to help us. Also, recall, that we want the modification to be permanent, so to accomplish this we can assign the column of the original `DataFrame` to the result of `str.replace()` call on that same column. \n",
    "\n",
    "\n",
    "```python\n",
    ">>> spending_df.loc[:, \"spending\"] = spending_df.loc[:, \"spending\"].str.replace(\"$\", \"\")\n",
    ">>> spending_df.loc[:, \"spending\"]  = spending_df.loc[:, \"spending\"].str.replace(\",\", \"\")\n",
    ">>> spending_df.head()\n",
    "            doctor_id          specialty       medication  nb_beneficiaries spending\n",
    "unique_id                                                                           \n",
    "BK982218   1750389599  INTERNAL MEDICINE     AZITHROMYCIN              12.0    77.26\n",
    "CG916968   1952344418         CARDIOLOGY      SIMVASTATIN              85.0   767.83\n",
    "SA964720   1669522744  INTERNAL MEDICINE  INSULIN DETEMIR              14.0  5409.29\n",
    "```\n",
    "\n",
    "We see that the code above made the desired changes and the `spending` column now looks like a proper `float64`. The final step is to convert the spending column into the float64 data type as it is now float-compatible. Recall that to do this we use the `astype() Series` method and pass it the string `float64` specifying the datatype. Also,  since we want this to be permanent, we save the result of the `astype()` method to the original `DataFrame's` 'spending' column.\n",
    "\n",
    "```python\n",
    ">>> spending_df.loc[:, \"spending\"] = spending_df.loc[:, \"spending\"].astype(\"float64\")\n",
    ">>> spending_df.dtypes\n",
    "doctor_id            object\n",
    "specialty            object\n",
    "medication           object\n",
    "nb_beneficiaries    float64\n",
    "spending            float64\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "We see from the results above that the `spending` column is now succesfully saved as a `pandas` `float64` data type. \n",
    "\n",
    "The entire procedure could be executed with a single line of code using method chaining as seen in the code below. Remember that we read method chaining from left to right, i.e. the leftmost method is executed first and the returned result is then the caller of the next method in the chain.\n",
    "\n",
    "```python\n",
    ">>> spending_df.loc[:, \"spending\"] = (spending_df.loc[:, \"spending\"]\n",
    "                               .str.replace(\"$\", \"\")\n",
    "                               .str.replace(\",\", \"\")\n",
    "                               .astype(\"float64\"))\n",
    "```\n",
    "\n",
    "Wrapping the method chaining calls in parenthesis is not necessary but allows us to write the complete expression across multiple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVJGCSZ1R8YE"
   },
   "outputs": [],
   "source": [
    "spending_df[\"spending\"] = (spending_df[\"spending\"]\n",
    "                           .str.replace(\"$\", \"\")\n",
    "                           .str.replace(\",\", \"\")\n",
    "                           .astype(\"float64\"))\n",
    "spending_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhkAynN4LxMR"
   },
   "source": [
    "# Exercise 5.2: `Series` String Methods\n",
    "\n",
    "Given the  `Series`, `UH_Series` which can be described by the following table:\n",
    "\n",
    "| Index |Data |\n",
    "|:----------|-----------|\n",
    "| 0 | 'U H Manoa' |\n",
    "| 1 | 'UH_Manoa' |\n",
    "| 2 | 'UH Manoa' |\n",
    "| 3 | 'UH-Manoa' |\n",
    "\n",
    "And can be constructed using the code shown below:\n",
    "\n",
    "```python\n",
    "UH_Series = pd.Series(['U H Manoa',  'UH_Manoa', 'UH Manoa', 'UH-Manoa'])\n",
    "```\n",
    "\n",
    "Which line of code will modify `UH_Series`, inplace, so that each entry matches the string: 'UHManoa'?\n",
    "\n",
    "A:\n",
    "```python\n",
    "(UH_Series\n",
    " .str.replace(\" \", \"\")\n",
    " .str.replace(\"_\",\"\")\n",
    ".str.replace(\"-\",\"\"))\n",
    "```\n",
    "\n",
    "B: \n",
    "```python\n",
    "(UH_Series\n",
    " .str.replace(\"_\",\" \")\n",
    ".str.replace(\"-\",\" \"))\n",
    "```\n",
    "\n",
    "C:\n",
    "```python\n",
    "UH_Series = (UH_Series\n",
    " .str.replace(\" \", \"\")\n",
    " .str.replace(\"_\",\"\")\n",
    ".str.replace(\"-\",\"\"))\n",
    "```\n",
    "\n",
    "D:\n",
    "```python\n",
    "UH_Series = (UH_Series\n",
    " .str.replace(\"_\",\"\")\n",
    ".str.replace(\"-\",\"\"))\n",
    "```\n",
    "\n",
    "*Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7Kal0TYNpLn"
   },
   "outputs": [],
   "source": [
    "# Exercise 4.2 Scratch code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-i_512i_Pyy1"
   },
   "source": [
    "# Reindexing\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The next, and very common, cleaning task that we will discuss is reindexing. To motivate, suppose that you are working with data that is not in an order you would like, it contains entries you are not concerned with, or maybe it does not contain all of the entries you desire. For example consider the following `DataFrame` `sine_wave_df` which is constructed by simulating a sine wave that may be measurements of the voltage signal coming from an outlet at your house. \n",
    "(do not concern yourself with how this `DataFrame` is constructed, the point of this example is to illustrate the value of reindexing):\n",
    "\n",
    "```python\n",
    ">>> times = [60.0 * 2 * np.pi * x / 900 for x in range(30)] \n",
    ">>> wave = np.sqrt(2) * 120.0 * np.sin(times) \n",
    "\n",
    ">>> sine_wave_df = pd.DataFrame({'wave':wave}, index=times)\n",
    ">>> sine_wave_df.sort_values(by='wave', inplace=True)\n",
    "```\n",
    "\n",
    "The above code constructs a `DataFrame` `sine_wave_df` that is indexed by the time and has the single column 'wave', which could be the voltage measurements of a home outlet. Currently the `DataFrame` `sine_wave_df` is sorted in increasing order by the values in the column `wave`. Let us try and plot this wave to see what is going on.\n",
    "\n",
    "```python\n",
    ">>> plt.figure()\n",
    ">>> sine_wave_df.plot(kind='line')\n",
    ">>> plt.show()\n",
    "```\n",
    "\n",
    "![](images/wave_before_reindex.png)\n",
    "\n",
    "The above image looks nothing like the expected sine wave vs. time signal we would expect to see from a home outlet. One solution to the problem would be to sort the `DataFrame` by the the time which, in this case, is the index, but, another solution would be to utilize the `reindex()` `DataFrame` method, and this option, as we will see is a little more flexible since we can simulatneously drop and add entries when we reindex, allowing us to do cool things like decimation and interpolation.\n",
    "\n",
    "The `DataFrame` `reindex()` method will create a new `DataFrame` with data entries that conform to the new index passed to the method. By conform I mean that the new `DataFrame` will have exactly the index passed to the method, the same order and the same labels. The entries that existed in the `DataFrame` which called the method that had labels that are in the new index will still exist in the new `DataFrame`, while those entries whose labels did not exist in the index passed will be dropped. Furthermore, if labels exist in the index passed to the `reindex()` method that did not exist in the calling `DataFrame`, then those entries will be filled with missing values, `NaN` unless otherwise specified by the caller.\n",
    "\n",
    "Let us reindex the `sine_wave_df` `DataFrame` by the same `times` array we made when we were constructing `sine_wave_df`, this will simply create a new `DataFrame` that is sorted in increasing time. To do this we call the `reindex()` `DataFrame` method passing the `times` array.\n",
    "\n",
    "```python\n",
    ">>> sine_wave_df_reindexed = sine_wave_df.reindex(times)\n",
    "```\n",
    "\n",
    "Now, let us plot the reindexed sine wave `DataFrame` to see if it is what we wanted.\n",
    "\n",
    "![](images/sine_wave_reindexed.png)\n",
    "\n",
    "Indeed the above plot looks like the expected sine wave. \n",
    "\n",
    "As stated, sorting is not the only value of reindexing, we may also add, or drop entries using this method. Adding entries to a `DataSet` like the sine wave we have been working with is called interpolating. By default, `reindex()` will set new entries to `NaN` to represent missing. But, we may also set these entries to a value using some filling function. The `reindex()` method has implemented for us a some common filling functions that we may specify when calling the method using the the optional `method` parameter, and those are *forward fill* and *backward fill*. Forward filling will propagate the last valid observation forward until the next valid entry, while backward fill will use the next valid entry to propagate backwards. Note that these methods may only be called on indices that are either only increasing or decreasing.\n",
    "\n",
    "To see how interpolation is done, let us reindex our sine wave `DataFrame` to conform to an index with times that are twice as close together, that is to say that the time between samples is halved, and forward fill the missing entries that arise. To do this we call `reindex()` on the `sine_wave_df` `DataFrame` and pass a new array with twice the frequency of the `times` array and set the `method` parameter to 'ffil'\n",
    "\n",
    "```python\n",
    ">>> times2 = [60.0 * 2 * np.pi * x / 1800 for x in range(60)]\n",
    ">>> sine_wave_df = sine_wave_df.reindex(times) \n",
    ">>> sine_wave_df_reindexed = sine_wave_df.reindex(times2, method='ffill')\n",
    "```\n",
    "\n",
    "To illustrate the results lets plot both the original `sine_wave_df` `DataFrame` and the new `sine_wave_df_reindexed` `DataFrame`. \n",
    "\n",
    "```python\n",
    ">>> sine_wave_df['time'] = times\n",
    ">>> sine_wave_df_reindexed['time'] = times2\n",
    ">>> fig, axes = plt.subplots(nrows=2, ncols=1) \n",
    ">>> sine_wave_df_reindexed.plot(kind='line', x='time', y='wave', ax=axes[1]) \n",
    ">>> sine_wave_df.plot(kind='line', x='time', y='wave', ax=axes[0]) \n",
    ">>> plt.show()  \n",
    "```\n",
    "\n",
    "![](images/interpolated_sine.png)\n",
    "\n",
    "We see that it looks as though there are steps in our new signal, and that is caused by the forward fill method for filling in the missing values, backward fill would have caused a very similar type of distortion. If this were voltage measurements from a home outlet then this type of distortion would be undersirable, a better method would be to interpolate using a linear method, that is filling the missing value with the arithemtic mean between the previous valid index and the next valid index. A linear method is not an option in the `reindex()` method, but it is in the `interpolate()` `DataFrame` method which we will discuss in a moment, but first let us change gears and look at reindexing and how it could be used on the medical spending data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TeoAy2wSy6A0"
   },
   "source": [
    "## Reindexing Continued: Example with medical spending data set\n",
    "\n",
    "We saw how reindexing could be used with time series data, now let us take a look at how the `reindex()` method would help us with analyzing the medical spending data set that we are more familiar with.\n",
    "\n",
    "Let us first take a peak into the `spending_df` `DataFrame` to remind ourselves how it is structured.\n",
    "\n",
    "```python\n",
    ">>> spending_df.head(n=3)\n",
    "            doctor_id          specialty       medication  nb_beneficiaries  spending\n",
    "unique_id                                                                            \n",
    "BK982218   1750389599  INTERNAL MEDICINE     AZITHROMYCIN              12.0    \\$77.26\n",
    "CG916968   1952344418         CARDIOLOGY      SIMVASTATIN              85.0   \\$767.83\n",
    "SA964720   1669522744  INTERNAL MEDICINE  INSULIN DETEMIR              14.0  \\$5409.29\n",
    "```\n",
    "\n",
    "We see that the `spending_df` `DataFrame` is indexed by the `unique_id` and has 5 columns labeled: 'doctor_id' 'specialty' 'medication' 'nb_beneficiaries' and 'spending'. Suppose we were only interested in the medications and not the doctors prescribing them, i.e. we only wanted the 2 columns: `medication` and `spending`. Furthermore, what if we wanted to add the column 'opioid_drug' that will, once we fill it with values, tell us whether the medication is classified as an opioid or not. To do this we could use the `reindex()` method. This time however we are reindexing the column of the `DataFrame`, so instead of passing an index we use the optional `columns` parameter of the `reindex()` method and set it to the columns we would like our new `DataFrame` to be labeled by.\n",
    "\n",
    "```python\n",
    ">>> spending_df.reindex(columns=['medication', 'spending', 'opioid_drug']).head(n=3)\n",
    "                  medication   spending opioid_drug\n",
    "unique_id                                       \n",
    "BK982218      AZITHROMYCIN    \\$77.26         NaN\n",
    "CG916968       SIMVASTATIN   \\$767.83         NaN\n",
    "SA964720   INSULIN DETEMIR  \\$5409.29         NaN\n",
    "```\n",
    "\n",
    "We see the returned `DataFrame` only contains the columns we specified and in the very same order. Also the new column `opioid_drug` was filled with, `NaNs`, missing values. We will learn how to fill theses missing values in the coming sections of this chapter.\n",
    "\n",
    "The `reindex()` `DataFrame` method has many parameters and we cannot cover them all but we did see the most important usages. I believe you will find the `reindex()` method very useful.  The table below summarizes the parameters of the `reindex()` method\n",
    "\n",
    "\n",
    "| Parameters |Description|\n",
    "|:----------|-----------|\n",
    "| `labels`| New labels / index to conform the axis specified by ‘axis’ to. |\n",
    "| `index, columns` | New labels / index to conform to. Preferably an Index object to avoid duplicating data |\n",
    "| `axis` | Axis to target. Can be either the axis name (‘index’, ‘columns’) or number (0, 1). |\n",
    "| `method` | method to use for filling holes in reindexed DataFrame.|\n",
    "| `copy` | Return a new object, even if the passed indexes are the same  |\n",
    "| `level` | Broadcast across a level, matching Index values on the passed MultiIndex level |\n",
    "| `fill_value` | Value to use for missing values. Defaults to NaN, but can be any “compatible” value |\n",
    "| `limit` | Maximum number of consecutive elements to forward or backward fill |\n",
    "| `tolerance` | Maximum distance between original and new labels for inexact matches.|\n",
    "\n",
    "\n",
    "\n",
    "* More information about the reindexing method is available [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIkMPJ0tBmXE"
   },
   "outputs": [],
   "source": [
    "spending_df.reindex(columns=['medication', 'spending', 'opiod_drug']).head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9S42DtET2q7H"
   },
   "source": [
    "# Exercise 5.3: Reindexing\n",
    "\n",
    "Given the following `DataFrame`, `df`, described in the table below:\n",
    "\n",
    "|  | color | number |\n",
    "|:----------|-----------|\n",
    "| d | green | 4 |\n",
    "| a | blue |1|\n",
    "| c | orange | 3 |\n",
    "\n",
    "And constructed using the following code:\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({'color':['green','blue', 'orange'], 'number': [4, 1, 3]}, \n",
    "                  index = ['d', 'a', 'c'])\n",
    "```\n",
    "\n",
    "Use the `reindex()` function to modify the `df DataFrame` inplace so that its new index is the list of labels `[a, b, c, d]` and the new values are left as `NaN`s. The `DataFrame` should have the following information and structure.\n",
    "\n",
    "|  | color | number |\n",
    "|:----------|-----------|\n",
    "| a | blue |1|\n",
    "| b | NaN | NaN |\n",
    "| c | orange | 3 |\n",
    "| d | green | 4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCcYFH0oIn-8"
   },
   "outputs": [],
   "source": [
    "# Type your solution to Exercise 4.3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmleAIb8QTBH"
   },
   "source": [
    "# Handling Missing Data\n",
    "\n",
    "It is very common for real world data to come with missing entries, and sometimes, as we saw with reindexing, we may introduce missing values ourselves, and if these entries are not identified and handled appropriately, then your analysis will be flawed.\n",
    "\n",
    "Handling missing data can be summarized into 3 objectives:\n",
    "\n",
    "1.   Identifying\n",
    "2.   Filtering\n",
    "3.   Filling\n",
    "\n",
    "For some datsets it may be more suitable to *filter* rows or columns with missing values, and for others it may be better to *fill* the entries, we will work through examples of both possibilites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCsc8QjfydXV"
   },
   "source": [
    "## Identifying\n",
    "\n",
    "We have learned in previous chapters that `pandas` denotes missing values with `NaN`. We have also seen how to load our data and specify what tags represent missing values using `read_table()` and `read_csv()` and the `na_values` parameter; this is the first step to identifying missing values. Next, once all the missing values are properly labeled as `NaN` in the `pandas` `DataFrame` or `Series`, it is often useful to find where all the missing values precisely are, i.e. the locations of the missing values. \n",
    "\n",
    "This is typically achieved with the  `isnull()` `DataFrame` and `Series` methods. `isnull()` returns `True` if a cell contains a `NaN` value, and returns `False` otherwise. There are no positional parameters that we can pass to this method.\n",
    "\n",
    "Let us see for instance the results of calling `isnull()` on the first four entries of the `spending_df` `DataFrame`\n",
    "\n",
    "```python\n",
    "spending_df.isnull().head(n=4)\n",
    "           doctor_id  specialty  medication  nb_beneficiaries  spending\n",
    "unique_id                                                              \n",
    "BK982218       False      False       False             False     False\n",
    "CG916968       False      False       False             False     False\n",
    "SA964720       False      False       False             False     False\n",
    "TR390895       False       True       False             False     False\n",
    "```\n",
    "\n",
    "We see that the `specialty` value for `unique_id` TR390895 evaluates `isnull()` to `True`. We will see soon that this is going to be useful information for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gm5ZIaM8qRV"
   },
   "outputs": [],
   "source": [
    "spending_df.isnull().head(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fF2jXG2ZyidR"
   },
   "source": [
    "## Filtering\n",
    "\n",
    "Filtering in this context of handling missing values refers to dropping the entries with missing values completely. Information is lost when you do this but in many practical cases this is acceptable. Various approaches can be taken to filtering out missing values, we will cover two, subsetting and the `dropna()` `DataFrame` method. \n",
    "\n",
    "To filter by subsetting, a skill we learned in chapter 3, we will make use of the fact that the `isnull()` `Series` method returns a boolean `Series` that is the exact same shape as the calling `Series`. Recall that `pandas` allows us to subset  `DataFrames` using boolean `Series`, the result will be a subset of the original `DataFrame` with only the entries whos index aligned with a True value. Also, recall that, ~, is a boolean operation that is equivalent to 'not', i.e. True is mapped to False and False is mapped to True. \n",
    "\n",
    "For example, we can filter out missing values in the `spending` `DataFrames` by first obtaining a boolean `Series` indicating whether the entry at the index had a valid entry in the `spending` column, i.e. if the entry was missing, `NaN`, then the corresponding entry in the boolean `Series` will be false. This is done by calling `isnull()` on the `spending` column of `spending_df` and 'notting' the result, i.e. we use the ~ operator. Then we use the boolean `Series` to subset `spending_df`. We can save our subset to a new `DataFrame` we will call `filtered_spending_df`\n",
    "\n",
    "```python\n",
    ">>> filtered_spending_df = spending_df.loc[~ spending_df.loc[:, \"spending\"].isnull(), : ]\n",
    "```\n",
    "\n",
    "To verify our results we can make use of the `Series` `sum()` method and make note that when calling the `sum()` method with a boolean `Series` we can consider `True` to be 1 and `False` to be 0. Thus, if we expect there to be no missing values in the `spending_df['spending']` `Series`, then we should expect calling `sum()` with the `Series` `filtered_spending_df.loc[:, 'spending'].isnull()` to be 0. Let us see if this is case.\n",
    "\n",
    "```python\n",
    ">>> filtered_spending_df.loc[:, 'spending'].isnull().sum()\n",
    "0\n",
    "```\n",
    "\n",
    "Excellent, the output above shows that there are 0 missing entries in the `spending` column of the `filtered_spending_df` `DataFrame` as we expected.\n",
    "\n",
    "`DataFrames` also have the `dropna()`method to drop entries with missing values. `dropna()` has the familiar optional parameter `axis` along which to drop the the column or the row that contains the `NaN`. If we set `axis=0` or `axis=\"rows\"` then we will drop rows containing `NaN`s. Similarly, if we set `axis=1` or `axis=\"columns\"` we will drop columns containing `NaN`s. Note that the operation does not overwrite the original data, but instead, returns a new `DataFrame` with the `NaN` dropped.\n",
    "\n",
    "![](images/axis_drop.png)\n",
    "\n",
    "The image above shows how the `dropna()` method will work with the `axis` parameter set to both 0 and 1 on the basic `DataFrame` with a single missing value, `NaN`.\n",
    "\n",
    "For example, lets make a new `DataFrame`, again we will call it `filtered_spending_df` which will overwrite the exisiting `DataFrame`, that is the same as the `spending_df` `DataFrame` except all the rows with missing values are dropped. To do this we use the `dropna()` method and set `axis=0`.\n",
    "\n",
    "```\n",
    ">>> filtered_spending_df = spending_df.dropna(axis=0)\n",
    "```\n",
    "\n",
    "The output of the `dropna()` method was saved into `filtered_spending_df`, lets verify our results. We expect there to be no missing values at all in the new `DataFrame`. To check this we can call the `sum()` `DataFrame` method with the `filtered_spending_df.isnull()` `DataFrame`. The output of the `sum()` `DataFrame` method will be a `Series` with an index matching the columns of `filtered_spending_df` and entries corresponding to the sum across the rows for the column.\n",
    "\n",
    "```python\n",
    ">>> filtered_spending_df.isnull().sum()\n",
    "doctor_id           0\n",
    "specialty           0\n",
    "medication          0\n",
    "nb_beneficiaries    0\n",
    "spending            0\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "The output tells us that the `dropna()` method worked as we expected, there are 0 missing values in each of the 5 rows of the `filtered_spending_df` `DataFrame`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Up7F0bjl-Aj7"
   },
   "outputs": [],
   "source": [
    "filtered_spending_df = spending_df.loc[~ spending_df.loc[:, \"spending\"].isnull(), : ]\n",
    "filtered_spending_df.loc[:, 'spending'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Y2D7B32aqlt"
   },
   "outputs": [],
   "source": [
    "filtered_spending_df = spending_df.dropna(axis=0)\n",
    "filtered_spending_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DdY_3vx-Ty0"
   },
   "source": [
    "### Filtering Cont.\n",
    "\n",
    "In addition to the parameter `axis`,  `dropna()` has other useful parameters that can we can use to customize the way we drop rows or columns from DataFrames, all of which are summarized in the table below.\n",
    "\n",
    "| Parameter | Description |\n",
    "|:----------:|:------------|\n",
    "| `how` | ('any') drops a row or a column if any of its value are `NaN`. <br/> ('all') drops a row or a column if all of its values are `NaN` | \n",
    "| `thresh` | Defines the minimum number of non-`NaN` required before a column is dropped. <br/> Useful for dropping `variables` (columns) with too many (above threshold ) `NaN`s |\n",
    "|`subset`| Defines a list of columns to consider. |\n",
    "|`inplace`| bool, default False. If True, do operation inplace and return None. |\n",
    "\n",
    "Notice that the familiar inplace parameter is available for the `dropna()` and is by default set to false (per usual). \n",
    "\n",
    "If you would like to read more about this method you can visit the `pandas` [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtjQ42dGM5Cl"
   },
   "source": [
    "# Exercise 5.4: Filtering Missing Values\n",
    "\n",
    "Consider the `DataFrame`, `flights_df`, described using the table below:\n",
    "\n",
    "|  | ORIGIN_AIRPORT | DESTINATION_AIRPORT | DEPARTURE_DELAY| ARRIVAL_DELAY |\n",
    "|:----------|-----------|---------|\n",
    "| 0 | HNL | SFO | 3.0 | -21.0 |\n",
    "| 1 | LAS | HNL | NaN | -2.0 |\n",
    "| 2 | HNL | ITO | NaN | NaN |\n",
    "| 3 | HNL | KOA| -6.0 | -9.0 |\n",
    "\n",
    "And constructed using the following line of code:\n",
    "\n",
    "```python\n",
    "flights_df = pd.DataFrame({'ORIGIN_AIRPORT': ['HNL', 'LAS', 'HNL', 'HNL'], \n",
    "                           'DESTINATION_AIRPORT': ['SFO', 'HNL', 'ITO', 'KOA'],\n",
    "                           'DEPARTURE_DELAY': [ 3., None, None, -6.], \n",
    "                           'ARRIVAL_DELAY': [-21.,  -2.,  None,  -9.]})\n",
    "```\n",
    "\n",
    "Which of the following statements will create a `DataFrame` with that is a subset of `flights_df` containing only ***rows*** that do not have ***both*** a missing `ARRIVAL_DELAY`  and `DEPARTURE_DELAY` value? (do not modify the original `flights_df DataFrame`).\n",
    "\n",
    "A:\n",
    "```python\n",
    "flights_df.dropna(axis='columns')\n",
    "```\n",
    "\n",
    "B:\n",
    "```python\n",
    "flights_df.dropna(axis='rows')\n",
    "```\n",
    "\n",
    "C:\n",
    "```python\n",
    "flights_df[ ~ (flights_df.loc[:, 'DEPARTURE_DELAY'].isnull() \n",
    "               & flights_df.loc[:, 'ARRIVAL_DELAY'].isnull())]\n",
    "```\n",
    "\n",
    "D:\n",
    "```python\n",
    "flights_df[ ~ (flights_df.loc[:, 'DEPARTURE_DELAY'].isnull() \n",
    "               | flights_df.loc[:, 'ARRIVAL_DELAY'].isnull())]\n",
    "```\n",
    "\n",
    "*Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPmXALiqEKIc"
   },
   "outputs": [],
   "source": [
    "# Exercise 4.4 Scratch code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w2zUWIeUyliB"
   },
   "source": [
    "## Filling\n",
    "\n",
    "Filling in this context of handling missing values refers to replacing the `NaNs` with valid values. This practice is sometimes also referred to as *imputation*, when we fill in missing data we are imputing the missing values. Filling data often requires some understanding of how your data was collected and or some domain knowledge so you can determine what is the appropriate way to fill the missing data. \n",
    "\n",
    "Two very broad categories that methods for filling missing values will fall into are:\n",
    "\n",
    "1.   Filling the value with a constant\n",
    "2.   Filling the value dynamically\n",
    "\n",
    "Both approaches can be carried out using the `DataFrame` and `Series` method `fillna()`, and the only difference between the two approaches will be the value we pass to `fillna()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Bv6CdF-_Nsw"
   },
   "source": [
    "### Filling Continued: `fillna()` with Static Values \n",
    "\n",
    "First lets discuss filling with a static, or constant, value.  The `fill_na()` `DataFrame` method will take either a scalar constant which replaces all missing values of the calling `DataFrame`, or a dictionary with specific values for each column of the calling `DataFrame`. `fill_na()` has the optional parameter `inplace` that is by default `False`. \n",
    "\n",
    "If we just wanted to make a new `DataFrame`, named `filled_spending_df`, that is a the same as `spending_df` but instead with all missing values set to 0, regardless of the location of the missing entry, then we would just pass the scalar constant 0. \n",
    "\n",
    "```python\n",
    ">>> filled_spending_df = spending_df.fillna(0)\n",
    "```\n",
    "\n",
    "We can verfiy our results by observing that the $3^{rd}$ and $4^{th}$ row entries of the $1^{st}$ column, `specialty`,  where originally missing in the `spending_df` `DataFrame`, so when we print those same entries in `filtered_df`we should see that they are 0.\n",
    "\n",
    "```python\n",
    ">>> filled_spending_df.iloc[[3,4], 1]\n",
    "unique_id\n",
    "TR390895    0\n",
    "JA436080    0\n",
    "Name: specialty, dtype: object\n",
    "```\n",
    "\n",
    "We see that the values that were once missing in `spending_df` are now 0.\n",
    "\n",
    "Most of the time, it is not the best practice to set all the missing values in a `DataFrame` to the same value. In our example a 0 in the `specialty` column does not make much sense, it would be more suitable to set the specialty to a string value such as `'UNKNOWN'`. We can accomplish column specific filling with static values by passing a `python` dictionary to the `fillna()` method. The dictionary should have keys that are the column labels of the calling `DataFrame`, and values that specify what you want to fill with. \n",
    "\n",
    "For example, let us make a new `DataFrame`, again named `filled_spending_df`,  where all missing entries in the `spending` or the `nb_beneficiaries` column are replaced with 0s, and all missing entries in the `specialty` column are replaced with the string `'UNKNOWN'`.  \n",
    "\n",
    "```python\n",
    ">>> filled_spending_df = spending_df.fillna( { \"specialty\": \"UNKNOWN\", \n",
    "                              \"nb_beneficiaries\": 0, \n",
    "                              \"spending\": 0 } )\n",
    "```\n",
    "\n",
    "To check our results we can take a look at the originally missing values in the `specialty` column of `spending_df`, we should see that they now all  hold the value `'UNKNOWN'`\n",
    "\n",
    "```python\n",
    ">>> filled_spending_df[spending_df.loc[:, 'specialty'].isnull()].loc[:, 'specialty']\n",
    "unique_id\n",
    "TR390895    UNKNOWN\n",
    "JA436080    UNKNOWN\n",
    "DT789371    UNKNOWN\n",
    "CF887728    UNKNOWN\n",
    "YN526335    UNKNOWN\n",
    "ES437458    UNKNOWN\n",
    "Name: specialty, dtype: object\n",
    "```\n",
    "\n",
    "Just as we expected, all the once missing values are now replaced with the string 'UNKNOWN' in the `specialty` column of the `DataFrame` `filled_spending_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSg2RA0ll5Fz"
   },
   "outputs": [],
   "source": [
    "filled_spending_df = spending_df.fillna(0)\n",
    "filled_spending_df.iloc[[3,4], 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pxYGtTexANRD"
   },
   "outputs": [],
   "source": [
    "filled_spending_df = spending_df.fillna( { \"specialty\": \"UNKNOWN\", \n",
    "                      \"nb_beneficiaries\": 0, \n",
    "                      \"spending\": 0 } )\n",
    "filled_spending_df[spending_df.loc[:, 'specialty'].isnull()].loc[:,'specialty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFg1XxeqC8d2"
   },
   "source": [
    "# Exercise 5.5: Filling Missing Entries with Static Values\n",
    "\n",
    "Again let us consider the `flights_df DataFrame` that was first introduced in exercise 5.4 and is described in the table below:\n",
    "\n",
    "|  | ORIGIN_AIRPORT | DESTINATION_AIRPORT | DEPARTURE_DELAY| ARRIVAL_DELAY |\n",
    "|:----------|-----------|---------|\n",
    "| 0 | HNL | SFO | 3.0 | -21.0 |\n",
    "| 1 | LAS | HNL | NaN | -2.0 |\n",
    "| 2 | HNL | ITO | NaN | NaN |\n",
    "| 3 | HNL | KOA| -6.0 | -9.0 |\n",
    "\n",
    "And constructed using the following line of code:\n",
    "\n",
    "```python\n",
    "flights_df = pd.DataFrame({'ORIGIN_AIRPORT': ['HNL', 'LAS', 'HNL', 'HNL'], \n",
    "                           'DESTINATION_AIRPORT': ['SFO', 'HNL', 'ITO', 'KOA'],\n",
    "                           'DEPARTURE_DELAY': [ 3., None, None, -6.], \n",
    "                           'ARRIVAL_DELAY': [-21.,  -2.,  None,  -9.]})\n",
    "```\n",
    "\n",
    "Write a command using the code cell below to fill the missing entries in both the `DEPARTURE_DELAY` and `ARRIVAL_DELAY` columns with 0. The original `DataFrame` should be modified, i.e. the changes should be done in place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLj99tJGEIcD"
   },
   "outputs": [],
   "source": [
    "# Type your solution to Exercise 4.5 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmPGR3ZdAVbZ"
   },
   "source": [
    "### Filling Continued: `fillna()` with Dynamic Values \n",
    "\n",
    "Dynamic filling refers to filling the missing entries with a values that  depends on the existing data. For example, one common strategy is filling missing values with a column's mean or median value.\n",
    "\n",
    "For instance, let us make a new `DataFrame`, again we will call it `filled_spending_df`, which is a copy of `spending_df` except that the mising values in the columns `nb_beneficiaries` and `spending` are replaced with their respective column means. The only difference between this and the static values case is that we need to first compute the values we are going to fill with. To do this we will utilize the `Series` `mean()` method, and then pass a dictionary to `fillna()` just as before.\n",
    "\n",
    "```python\n",
    ">>> average_spending = spending_df.loc[:, \"spending\"].mean()\n",
    ">>> average_nb_beneficiaries = spending_df.loc[:, \"nb_beneficiaries\"].mean()\n",
    ">>> filled_spending_df = spending_df.fillna({\"nb_beneficiaries\": average_nb_beneficiaries , \n",
    "                      \"spending\": average_spending})\n",
    "```\n",
    "\n",
    "Now lets check our results, \n",
    "\n",
    "```\n",
    "print(average_nb_beneficiaries)\n",
    "55.6\n",
    ">>> filled_spending_df.loc[spending_df.loc[:, 'nb_beneficiaries'].isnull(), 'nb_beneficiaries']\n",
    "unique_id\n",
    "EQ932492    55.6\n",
    "ON964391    55.6\n",
    "BT820276    55.6\n",
    "AD891213    55.6\n",
    "Name: nb_beneficiaries, dtype: float64\n",
    "\n",
    ">>> print(average_spending)\n",
    "5806.981\n",
    ">>> filled_spending_df.loc[spending_df.loc[:, 'spending'].isnull(), 'spending']\n",
    "unique_id\n",
    "WO822855    5806.981\n",
    "AD891213    5806.981\n",
    "ES437458    5806.981\n",
    "NM108377    5806.981\n",
    "Name: spending, dtype: float64\n",
    "```\n",
    "\n",
    "We see that all the once missing values in the `spending_df` `DataFrame` columns `spending` and `nb_beneficiaries` are now replaced by their respective column means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a63OYm5-BH0t"
   },
   "outputs": [],
   "source": [
    "average_spending = spending_df.loc[:, \"spending\"].mean()\n",
    "average_nb_beneficiaries = spending_df.loc[:, \"nb_beneficiaries\"].mean()\n",
    "filled_spending_df = spending_df.fillna({\"nb_beneficiaries\": average_nb_beneficiaries , \n",
    "                      \"spending\": average_spending})\n",
    "print('--------nb_beneficiaries---------')\n",
    "print(average_nb_beneficiaries)\n",
    "print(filled_spending_df.loc[spending_df.loc[:, 'nb_beneficiaries'].isnull(), 'nb_beneficiaries'])\n",
    "print('------------spending-------------')\n",
    "print(average_spending)\n",
    "print(filled_spending_df.loc[spending_df.loc[:, 'spending'].isnull(), 'spending'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3H2b-pDgESgV"
   },
   "source": [
    "### Filling Continued : `fillna()` with Dynamic Values 2\n",
    "\n",
    "Very similar to what we saw with reindexing, `fillna()` has a `method` parameter that can be modified to either back fill, `method='bfill'`, or forward fill, `method=ffill`, missing values along an axis. Recall that forward filling will propagate the last valid observation forward until the next valid entry, while backward fill will use the next valid entry to propagate backwards.\n",
    "\n",
    "For example, lets us forward fill the `DataFrame` `spending_df` and save the result to `ffill_spending_df`. Since it really only make sense to forward fill across rows we will set `axis='rows'`. To verify the results we will also print the head of the `specialy` column of both the `ffill_spending_df` `DataFrame` and the original `spending_df` `DataFrame` for reference.\n",
    "\n",
    "```python\n",
    ">>> ffill_spending_df = spending_df.fillna(method='ffill', axis='rows')\n",
    ">>> print(spending_df.loc[:, 'specialty'].head())\n",
    "unique_id\n",
    "BK982218    INTERNAL MEDICINE\n",
    "CG916968           CARDIOLOGY\n",
    "SA964720    INTERNAL MEDICINE\n",
    "TR390895                  NaN\n",
    "JA436080                  NaN\n",
    "Name: specialty, dtype: object\n",
    ">>> print(ffill_spending_df.loc[:, 'specialty'].head())\n",
    "unique_id\n",
    "BK982218    INTERNAL MEDICINE\n",
    "CG916968           CARDIOLOGY\n",
    "SA964720    INTERNAL MEDICINE\n",
    "TR390895    INTERNAL MEDICINE\n",
    "JA436080    INTERNAL MEDICINE\n",
    "Name: specialty, dtype: object\n",
    "```\n",
    "\n",
    "We see in the example above that the resulting `DataFrame` from the `fillna()` method has its missing values filled by the nearest previous valid entry, 'INTERNAL MEDICINE' . \n",
    "\n",
    "Let us do the same operation except set `method='bfill'`. We will save the results of the `fillna()` method call to the `DataFrame` `bfill_spending_df` and verify our results in the same way.\n",
    "\n",
    "```python\n",
    ">>> spending_df.fillna(method='bfill')\n",
    ">>> print(spending_df.loc[:, 'specialty'].head())\n",
    "unique_id\n",
    "BK982218    INTERNAL MEDICINE\n",
    "CG916968           CARDIOLOGY\n",
    "SA964720    INTERNAL MEDICINE\n",
    "TR390895                  NaN\n",
    "JA436080                  NaN\n",
    "Name: specialty, dtype: object\n",
    ">>> print(bfill_spending_df.loc[:, 'specialty'].head())\n",
    "unique_id\n",
    "BK982218            INTERNAL MEDICINE\n",
    "CG916968                   CARDIOLOGY\n",
    "SA964720            INTERNAL MEDICINE\n",
    "TR390895    INTERVENTIONAL CARDIOLOGY\n",
    "JA436080    INTERVENTIONAL CARDIOLOGY\n",
    "Name: specialty, dtype: object\n",
    "```\n",
    "\n",
    "Now the missing values are different, they are 'INTERVENTIONAL CARDIOLOGY', since backward fill will use the next valid entry to propagate backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9wKnoAaGMoF"
   },
   "outputs": [],
   "source": [
    "ffill_spending_df = spending_df.fillna(method='ffill', axis='rows')\n",
    "print(spending_df.loc[:, 'specialty'].head())\n",
    "print(ffill_spending_df.loc[:, 'specialty'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBi7KhUyv03N"
   },
   "outputs": [],
   "source": [
    "bfill_spending_df = spending_df.fillna(method='bfill', axis='rows')\n",
    "print(spending_df.loc[:, 'specialty'].head())\n",
    "print(bfill_spending_df.loc[:, 'specialty'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QGoJO_7HJ5E"
   },
   "source": [
    "# Exercise 5.6: Filling Missing Entries with Dynamic Values\n",
    "\n",
    "We will utilize the `flights_df DataFrame` one last time. Recall that it contains the information described in the table below:\n",
    "\n",
    "|  | ORIGIN_AIRPORT | DESTINATION_AIRPORT | DEPARTURE_DELAY| ARRIVAL_DELAY |\n",
    "|:----------|-----------|---------|\n",
    "| 0 | HNL | SFO | 3.0 | -21.0 |\n",
    "| 1 | LAS | HNL | NaN | -2.0 |\n",
    "| 2 | HNL | ITO | NaN | NaN |\n",
    "| 3 | HNL | KOA| -6.0 | -9.0 |\n",
    "\n",
    "And can be constructed using the following line of code:\n",
    "\n",
    "```python\n",
    "flights_df = pd.DataFrame({'ORIGIN_AIRPORT': ['HNL', 'LAS', 'HNL', 'HNL'], \n",
    "                           'DESTINATION_AIRPORT': ['SFO', 'HNL', 'ITO', 'KOA'],\n",
    "                           'DEPARTURE_DELAY': [ 3., None, None, -6.], \n",
    "                           'ARRIVAL_DELAY': [-21.,  -2.,  None,  -9.]})\n",
    "```\n",
    "\n",
    "Write a command using the code cell below to fill the missing entries in both the `DEPARTURE_DELAY` and `ARRIVAL_DELAY` columns with the column medians. The original `DataFrame` should **not** be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AnZw14C4IKsF"
   },
   "outputs": [],
   "source": [
    "# Type your solution to Exercise 4.6 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kn0fz6wtv8mi"
   },
   "source": [
    "### Filling Continued: The `interpolate()` `DataFrame` method\n",
    "\n",
    "Interpolation is the construction of new data points from surrounding data points. Forward filling and backward filling are examples of interpolation methods, but there are many more. Interpolation is typically useful in time series data, that is data that is taken periodically and organized in chronological order.\n",
    "\n",
    "The `interpolate()` `DataFrame` method will fill missing values in the calling `DataFrame` using an interpolation method specified with the `method` parameter across the axis specifieid by the `axis` parameter. We have multiple options to set the `method` parameter to and all of them can be seen [here](http://pandas.pydata.org/pandas-docs/version/0.16.2/generated/pandas.DataFrame.interpolate.html), but we will be focussing on 'linear' interpolation. \n",
    "\n",
    "To see the value of interpolation we will reuse the same `sine_wave_df` from the reindexing section. Previously, we interpolated the missing values introduced by reindexing by forward filling, which introduced some undesired distortion. Now we will interpolate the missing values using the linear method.\n",
    "\n",
    "```python\n",
    ">>> sine_wave_df_reindexed = sine_wave_df.reindex(times2)\n",
    ">>> sine_wave_df_reindexed.interpolate(method='linear', inplace=True)\n",
    "```\n",
    "\n",
    "Now lets again plot  both the original `sine_wave_df` `DataFrame` and the new `sine_wave_df_reindexed` `DataFrame`, this time however we will use a sactter plot to better illustrate the value of interpolation.\n",
    "\n",
    "```python\n",
    ">>> sine_wave_df['time'] = times\n",
    ">>> sine_wave_df_reindexed['time'] = times2\n",
    ">>> fig, axes = plt.subplots(nrows=2, ncols=1) \n",
    ">>> sine_wave_df_reindexed.plot(kind='scatter', x='time', y='wave', ax=axes[1]) \n",
    ">>> sine_wave_df.plot(kind='scatter', x='time', y='wave', ax=axes[0]) \n",
    ">>> fig.show()\n",
    "```\n",
    "\n",
    "![](images/sine_wave_linear_interpolate.png)\n",
    "\n",
    "We see in the image above that now the resulting waveform is much smoother. Note that there is still some distortion in this signal and it is not a perfect interpolation, but for this application it is visibly better that forward filling and backward filling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQg_y6x6Bayy"
   },
   "source": [
    "## Handling Missing Data for the Medical Spending Data Set\n",
    "\n",
    "Let us take a look at how we can use the skills we learned for handling missing data can be applied to our running example of the Medical Spending Data Set. First, to gain some perspective and start to get an idea of what we are working with, we should check exactly how many values are missing from each column in the data set. To do this we will check call `sum()` with the `DataFrame` `spending_df.isnull()`. Recall that the output of the `sum()` `DataFrame` method will be a `Series` with an index matching the columns of the calling `DataFrame` and entries corresponding to the sum across the rows for the column.\n",
    "\n",
    "```python\n",
    ">>> spending_df.isnull().sum()\n",
    "doctor_id           0\n",
    "specialty           6\n",
    "medication          1\n",
    "nb_beneficiaries    4\n",
    "spending            4\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "From the output above, it looks like every column besides doctor_id has missing values. Our next task will is to decide how we should handle these missing entries. Suppose our study is primarily interested in investigating the spending of each specialty, that is spending and specialty are the most valuable features for us, then one reasonable solution would be to drop the rows where `specialty`  or `spending` is missing, replace missing `nb_beneficiaries` with the average, and replace the missing `medication` entry with a new values \"UNKNOWN\".\n",
    "\n",
    "To do this we will need to do both filtering and filling. An efficient way to accomplish our goal would be to utilize the `dropna()` `DataFrame` method and the `fillna()` `DataFrame` method. We need to be careful with the `dropna()` method since by default it will consider and drop any rows that has a missing value in any column. If we want `dropna()` to only be concerned with a subset of the columns of the calling `DataFrame` then we can specify that using the `subset` parameter. Thus since we want to only drop rows if they have a missing value in the `specialty`  or `spending` columns we will set `subset=['specialty', 'spending']` . We can then call `fillna()` with the returned `DataFrame` from the `dropna()` call and pass it a dictionary with the desired values static and dynamic values. We want this change to be permanent so we can save the result into the `DataFrame` `spending_df`. Finally, check our results we can again call `sum()` with the `DataFrame` `spending_df.isnull()`.\n",
    "\n",
    "```python\n",
    ">>> spending_df = spending_df.dropna(subset=[\"specialty\",\"spending\"]).fillna({\n",
    "                    \"nb_beneficiaries\": spending_df[\"nb_beneficiaries\"].mean(), \n",
    "                    \"medication\": \"UNKNOWN\" } )\n",
    ">>> spending_df.isnull().sum()\n",
    "doctor_id           0\n",
    "specialty           0\n",
    "medication          0\n",
    "nb_beneficiaries    0\n",
    "spending            0\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "As desired, there are no more missing entries in our `spending_df` `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xENHhcykJ1cg"
   },
   "outputs": [],
   "source": [
    "print(spending_df.isnull().sum())\n",
    "print(\"----------------------\")\n",
    "spending_df = spending_df.dropna(subset=[\"specialty\",\"spending\"]).fillna({\n",
    "                    \"nb_beneficiaries\": spending_df[\"nb_beneficiaries\"].mean(), \n",
    "                    \"medication\": \"UNKNOWN\" } )\n",
    "\n",
    "spending_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9pQBcS4QQQC"
   },
   "source": [
    "# Function Application and Mapping\n",
    "\n",
    "---\n",
    "\n",
    "Function application and mapping simply refers to proccessing the entries of a `DataFrame` to better suite your needs.\n",
    "\n",
    "Function application falls into one of two categories:\n",
    "\n",
    "1. **Global Processing**\n",
    "2. **Group Specific Processing**\n",
    "\n",
    "Global processing is applying the same function to every entry in a `Series` or `DataFrame`, while group specific processing is applying functions to entries that belong to a certain group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sHGSMIrMeeGK"
   },
   "source": [
    "## Global Processing\n",
    "\n",
    "To apply a function to every element in a `DataFrame` we can use the `applymap()` `DataFrame` method.  The `applymap()` method is a function which takes one positional argument as input and that is a callable function which takes a single value and returns a single value. The `applymap()` method will apply the function passed to every single entry in the calling `DataFrame` and return a new `DataFrame` with the processed entries.\n",
    "\n",
    "Let us see a simple example. We will construct a `DataFrame` `df` that is $3 \\times 3$, i.e. there are three rows and three columns. The entries will be consecutive multiples of 3. To each entry we will apply the anonymous function: `lambda x: x / 3` which will divide a given input by 3. The result will be a new $3 \\times 3$ `DataFrame` with the same index and columns as the caller with entries that are the results of the passed function.\n",
    "\n",
    "```python\n",
    ">>> df = pd.DataFrame([[ 0,  3,  6], [ 9, 12, 15], [18, 21, 24]], columns=['a', 'b', 'c'])\n",
    ">>> df.applymap(lambda x: x / 3)\n",
    "     a    b    c\n",
    "0  0.0  1.0  2.0\n",
    "1  3.0  4.0  5.0\n",
    "2  6.0  7.0  8.0\n",
    "```\n",
    "\n",
    "This type of processing is relatively rare since it should be the case that it makes sense to apply the same function to every entry regardless of its location. For example, in the `spending_df` `DataFrame` there are few functions that would be reasonable to apply globally. But when you need this functionality, the `applymap()` function is quite useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvT6lE9lxJfo"
   },
   "source": [
    "## Group Specific Processing\n",
    "\n",
    "A common scenario is applying a function to a specific group of data. By group of data I mean a subset of the data that is the same based on a criterion. \n",
    "\n",
    "The `groupby()` `DataFrame` method is used to group rows of data by one or more of the column entries . The `groupby()` method accepts the parameter `by` which specifies how you want to group the rows of the calling `DataFrame`. `by` can be a single column label, a list of column lables, or a callable function. The method will return a `pandas` `GroupBy` object, an object we have not seen before. This object has certain attributes and methods that will be useful to us. In this chapter we will only cover the case of setting the `by` parameter of the `groupby()` method to a single column entry, if you are interested you can read more about the method [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html). \n",
    "\n",
    "If `by` is a sinlge label then the calling `DataFrame` will be grouped by the values in the column with the passed label, i.e. every entry with the same value in the specified column will be in the same group. For example, let us group the `spending_df` `DataFrame` by the values in the `specialty` column and save the returned `GroupBy` object to the variable we will call `spending_by_specialty`. To do this we use the following code.\n",
    "\n",
    "```python\n",
    ">>> spending_by_specialty = spending_df.groupby('specialty')\n",
    "```\n",
    "\n",
    "`GroupBy` objects have a handy method called `get_group()`, which returns all the entries of a specified group as a `DataFrame`. The `get_group()` method will take a positional argument that is the name of the group to access. Then the method returns a `DataFrame`, which is a subset of the initial `DataFrame` used to instantiate the `GroupBy` object. The entries of the returned `DataFrame` are all those entries in the column specified by the `by` parameter in the original `groupby()` call that match the name used in the `get_group()` call.\n",
    "\n",
    "Continuing with the example of the `spending_by_specialty` `GroupBy` object, let us see how we would retrieve the group of rows from the `spending_df` `DataFrame` whos entries in the `specialty` column were all the same value of 'CARDIOLOGY'. This group will conviently have the name 'CARDIOLOGY', thus when we use the `get_group` method we will simply pass the value 'CARDIOOGY'.\n",
    "\n",
    "```python\n",
    ">>> spending_by_specialty.get_group(\"CARDIOLOGY\")\n",
    "            doctor_id   specialty            medication  nb_beneficiaries  spending\n",
    "unique_id                                                                          \n",
    "CG916968   1952344418  CARDIOLOGY           SIMVASTATIN              85.0    767.83\n",
    "CG865025   1497955603  CARDIOLOGY  ATORVASTATIN CALCIUM              82.0   2726.72\n",
    "MK361461   1245206184  CARDIOLOGY   PANTOPRAZOLE SODIUM             102.0   1608.42\n",
    "YS123432   1841589744  CARDIOLOGY       VENLAFAXINE HCL              25.0    265.49\n",
    "```\n",
    "\n",
    "The example above returns all the entries of the  \"CARDIOLOGY\" group, i.e. all the entries from the original  `DataFrame` whos entries in the `specialty` column is \"CARDIOLOGY\", organized into a new `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_XL90pN1Xsc"
   },
   "outputs": [],
   "source": [
    "spending_by_specialty = spending_df.groupby('specialty')\n",
    "spending_by_specialty.get_group(\"CARDIOLOGY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlMI5-EJUeFg"
   },
   "source": [
    "# Exercise 5.7: Group Specific Processing\n",
    "\n",
    "Which of the following options will correctly group data in `HNL_flights_df` by the entries in the `ORIGIN_AIRPORT` column and save the results in a `GroupBy` object, `HNL_flights_by_origin`?\n",
    "\n",
    "A:\n",
    "```python\n",
    "HNL_flights_by_origin = HNL_flights_df.groupby('ORIGIN_AIRPORT')\n",
    "```\n",
    "\n",
    "B:\n",
    "```python\n",
    "HNL_flights_df.groupby('ORIGIN_AIRPORT', inplace=True)\n",
    "```\n",
    "\n",
    "C:\n",
    "```python\n",
    "HNL_flights_by_origin.groupby('ORIGIN_AIRPORT')\n",
    "```\n",
    "\n",
    "D:\n",
    "```python\n",
    "HNL_flights_by_origin = pd.groupby(HNL_flights_df['ORIGIN_AIRPORT'])\n",
    "```\n",
    "\n",
    "*Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown.*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Building on the first part of this exercise, which of the following lines of code will extract the subset, or group, of data in `HNL_flights_df` that all have a common `ORIGIN_AIRPORT` value of `LAX` and save the result into the `DataFrame: LAX_to_HNL_df`?\n",
    "\n",
    "\n",
    "A:\n",
    "```python\n",
    "LAX_to_HNL_df = HNL_flights_by_origin.get_group(ORIGIN_AIRPORT = 'LAX')\n",
    "```\n",
    "\n",
    "B:\n",
    "```python\n",
    "LAX_to_HNL_df = HNL_flights_by_origin.loc[:,'LAX']\n",
    "```\n",
    "\n",
    "C:\n",
    "```python\n",
    "LAX_to_HNL_df = HNL_flights_by_origin['LAX']\n",
    "```\n",
    "\n",
    "D:\n",
    "```python\n",
    "LAX_to_HNL_df = HNL_flights_by_origin.get_group('LAX')\n",
    "```\n",
    "\n",
    "*Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRujZDErXa_y"
   },
   "outputs": [],
   "source": [
    "# Exercise 4.7 Scratch code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGMMA2T9xRTP"
   },
   "source": [
    "### Split Apply Combine\n",
    "\n",
    "Getting groups can be easily implemented using subsetting. For instance, we could have obtained the \"CARDIOLOGY\" group of the `spending_df` `DataFrame` by subsetting `spending_df` with the boolean `Series` returned from the operation: `spending_df.loc[:, \"specialty\"] == \"CARDIOLOGY\"`. \n",
    "\n",
    "```python\n",
    "spending_df[spending_df.loc[:, \"specialty\"] == \"CARDIOLOGY\"]\n",
    "            doctor_id   specialty            medication  nb_beneficiaries  spending\n",
    "unique_id                                                                          \n",
    "CG916968   1952344418  CARDIOLOGY           SIMVASTATIN              85.0    767.83\n",
    "CG865025   1497955603  CARDIOLOGY  ATORVASTATIN CALCIUM              82.0   2726.72\n",
    "MK361461   1245206184  CARDIOLOGY   PANTOPRAZOLE SODIUM             102.0   1608.42\n",
    "YS123432   1841589744  CARDIOLOGY       VENLAFAXINE HCL              25.0    265.49\n",
    "```\n",
    "\n",
    "We see in the above example that the returned `DataFrame` is exactly the same as the result we saw in the previous cell introducing `groupby()` and `get_group()`. So why use `GroupBy` objects anyway?\n",
    "\n",
    "An ideal usage of `groupby()`, and the resulting `GroupBy object`, will apply operations to **each** group independently. Furthermore, `GroupBy` objects are intended to be applied in the context of the data processing paradigm called \"split-apply-combine\"\n",
    "\n",
    "* **Split** the data into chunks defined using one or more columns\n",
    "* **Apply** some operation on the chunks generated. \n",
    "* **Combine** the results of the applied operation into a new `DataFrame`\n",
    "\n",
    "For instance, suppose we wanted to compute the total spending by `specialty`and save the result to a news `DataFrame`, the steps we would need to take are:\n",
    "\n",
    "1. Split the data by `specialty`, i.e. `groupby('specialty')`\n",
    "2. Apply the `sum()` method to the `spending` column for each group\n",
    "3. Combine the results from each group into a new `DataFrame`\n",
    "\n",
    "![](images/split_apply_combine_example.png)\n",
    "\n",
    "So rather than manually subsetting each group and then applying the desired operation we could automate this workflow using the helpful `GroupBy` methods implemented by `pandas` to save ourselves some time and effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5hv9GKf5RMw"
   },
   "source": [
    "#### The 3 Classes of Opearations on Groups\n",
    "\n",
    "There are 4 classes of split-apply-combine operations that can be applied to group data.\n",
    "\n",
    "\n",
    "1. __Aggregations__ generate a single value for each group\n",
    "  \n",
    "2.  __Transformations__ convert the data and generate a group of the same size as the original group.\n",
    "\n",
    "3.  __Filters__ retain or discard a group based on group-specific boolean computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fBi2KE7_Ipb"
   },
   "source": [
    "![](./images/aggregate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjY1ne0W_IeZ"
   },
   "source": [
    "![](./images/transform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7p2JxIT_ISo"
   },
   "source": [
    "![](images/filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtvBOY2DxxoG"
   },
   "source": [
    "#### Aggregate\n",
    "\n",
    "__Aggregations__ aggregate the data in each group, i.e., they reduce the data to a single value. This includes, for instance, computing group sums, means, maximums, minimums, _etc_. Some of the interesting/important summary aggregation methods of `GroupBy`  objects are:\n",
    "\n",
    "|Methods| Decription|\n",
    "|:----------|:----------------|\n",
    "| `mean`, `median` | Computes the mean and the median in each group| \n",
    "| `min` , `max` | computes the min and max in each group| \n",
    "| `size` | computes the number of values in each group| \n",
    "\n",
    "When one of these methods are called by the `GroupBy` object, they are applied to each group individually and then the group is combined into a new `DataFrame`.\n",
    "\n",
    "For example suppose we wanted to group by `specialty`, apply the `sum()` method to calculate the total `spending` and total `nb_beneficiaries`, and then combine the results into a new `DataFrame` which holds the total `spending` and `nb_beneficiaries` by `specialty`. We could achieve this by first splitting the data using the `groupby()` `DataFrame` method to obtain a new `GroupBy` object, we will call it `spending_by_specialty`. Then we could apply and combine using the `GroupBy` object's `sum()` method.\n",
    "\n",
    "```python\n",
    ">>> spending_by_specialty = spending_df.groupby('specialty')\n",
    ">>> spending_by_specialty.sum().head(n=3)\n",
    "                 nb_beneficiaries      spending\n",
    "specialty                                      \n",
    "CARDIOLOGY              73.500000   1342.115000\n",
    "ENDOCRINOLOGY          105.000000  76346.720000\n",
    "FAMILY PRACTICE         71.461538   3318.526154\n",
    "```\n",
    "\n",
    "We see from the above example that the `GroupBy` `sum()` method returns a `DataFrame` with an index labeling the group that the row entry corresponds to and entries telling us the total `spending` and total `nb_beneficiaries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSD3S1kcvXIh"
   },
   "outputs": [],
   "source": [
    "spending_df.groupby('specialty').sum().head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODHG9if8ycUR"
   },
   "source": [
    "##### Aggregate Continued\n",
    "\n",
    "As discussed in the previous cell, `pandas` has implemented for us the most common aggregate methods for us, like `sum()` and `mean()`, but sometimes our data requires unique processing. The `GroupBy` method `agg()` can be used where complex or custom aggregation logic is required. The method `agg()` will take a function and use it to aggregate the group in the same way that we saw `sum()` do in the previous cell. The function passed must take a `DataFrame` as an argument, and that passed `DataFrame` will be each group of the calling `GroupBy` object.\n",
    "\n",
    "For example, suppose we wanted to find the total spending by specialty in Canadian dollars. We can define a function called `sum_spending_CAD()` to return the sum of the spending of a group in Canadian Dollars. Then we can create a new `GroupBy` object, call it `spending_by_specialty`, using a subset of the `spending_df` `DataFrame` only containing the `specialty` and `spending` columns. Lastly, we can can call `agg()` with the `spending_by_specialty` `GroupBy` object and pass it the `sum_spending_CAD` function.\n",
    "\n",
    "```python\n",
    ">>> def sum_spending_CAD(x):\n",
    ">>>    return x.sum() * 1.33\n",
    ">>> spending_by_specialty = spending_df.loc[:, ['specialty', 'spending']].groupby('specialty')\n",
    ">>> spending_by_specialty.agg(sum_spending_CAD).head(n=3)\n",
    "                   spending\n",
    "specialty                  \n",
    "CARDIOLOGY        6764.2596\n",
    "ENDOCRINOLOGY    96196.8672\n",
    "FAMILY PRACTICE  54357.4584\n",
    "```\n",
    "\n",
    "We see in the above example that the result is a new `DataFrame` with the unique `speciality` values as the index and values corresponding the sum total of the spending by specialty in Candian dollars.\n",
    "\n",
    "To customize group specific processing even further `agg()` can also take a dictionary of functions to aggregate on. The dictionary should be the name of a column of the group and the value a callable function that will take a `Series`. \n",
    "\n",
    "For example, suppose we wanted to create a new `DataFrame` that tells us the total `nb_beneficiaries` and the max `spending` by specialty from `spending_df`. To do this we would first `groupby()` `specialty` and then call `agg()` with the new `GroupBy` object, passing it the dictionary: `{'nb_beneficiaries' :sum,'spending' : max}`, which specifies that we want to sum the `nb_beneficiaries` column and find the max of the `spending` column.\n",
    "\n",
    "```python \n",
    ">>> spending_by_specialty = spending_df.groupby('specialty')\n",
    ">>> spending_by_specialty.agg({'nb_beneficiaries' :sum,\n",
    "                                 'spending' : max}).head(n=3)\n",
    "                 nb_beneficiaries  spending\n",
    "specialty                                  \n",
    "CARDIOLOGY                  294.0   2726.72\n",
    "ENDOCRINOLOGY               105.0  76346.72\n",
    "FAMILY PRACTICE             929.0  15640.59\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3gg8vN5AzQ3G"
   },
   "outputs": [],
   "source": [
    "def sum_spending_CAD(x):\n",
    "   return x.sum() * 1.29\n",
    "spending_by_specialty = spending_df.loc[:, ['specialty', 'spending']].groupby('specialty')\n",
    "spending_by_specialty.agg(sum_spending_CAD).head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uebGVP6KV0tc"
   },
   "source": [
    "# Exercise 5.8: Aggregate\n",
    "\n",
    "Using the code cell below, create a new `DataFrame` named `delay_by_origin` that is indexed by the unique origin airports in `HNL_flights_df` and contains the median departure and arrival delays for groups of flights with common origin airports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOWb2e3iYPRX"
   },
   "outputs": [],
   "source": [
    "# Type your solution to Exercise 4.8 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhxoqxboyBdG"
   },
   "source": [
    "#### Transform\n",
    "\n",
    " __Transformations__ change the data in a way that is group-specific. As opposed to aggregations, which reduce the data into a single value, transformations modify the data but don't change the shape of the groups\n",
    "\n",
    "The example below computes the percent contribution of each entry to each specialty by applying a transformation that normalizes the entry's spending over the total spending in that specialty. \n",
    "\n",
    "![](images/transform_spending.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWc7L9-U1BXr"
   },
   "source": [
    "##### Transform Continued 1\n",
    "\n",
    "Applying a transformation is done using the `transform()` `GroupBy` method. The `transform()` method takes as input a function name, which it calls on each group of the `GroupBy` object. The function passed to `transform()` must take a `DataFrame`, which will be a group of the calling `GroupBy` object. \n",
    "\n",
    "For example, suppose we wanted to transform the `spending` column of the `spending_df` `DataFrame` to hold the percentage of the total spending by specialty that rows makes up.  First, we would define a function which will take a `DataFrame` and calculate the the percentage of the total each entry takes up. Then we will create a new `GroupBy` object groupded by `specialty` from a subset of `spending_df` that only has the columns `spending` and `specialty`. Then we will call transform passing it the name of our defined function. \n",
    "\n",
    "```python\n",
    ">>> def my_function(x):\n",
    ">>>    return (x   / x.sum() ) * 100\n",
    "  \n",
    ">>> spending_by_specialty = spending_df.groupby('specialty')\n",
    " \n",
    ">>> spending_by_specialty.transform(my_function)[spending_df['specialty'] == \"CARDIOLOGY\"]\n",
    "\n",
    "            spending\n",
    "unique_id           \n",
    "CG916968   14.302612\n",
    "CG865025   50.791475\n",
    "MK361461   29.960547\n",
    "YS123432    4.945366\n",
    "```\n",
    "\n",
    "We see that the result is a new `DataFrame` with an index matching that of the original `DataFrame` used to initialize the `GroupBy` object. This is different than the aggregation example because aggregation reduces the group to a single value, while transformation maintains the shape of the calling `DataFrame`.\n",
    "\n",
    "Let us save these results into a new column in `spending_df` called `spending_pct`.\n",
    "\n",
    "```python\n",
    ">>> spending_df['spending_pct'] = spending_by_specialty.transform(my_function)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e64Euy5R1U-u"
   },
   "outputs": [],
   "source": [
    "def my_function(x):\n",
    "  return (x   / x.sum() ) * 100\n",
    "\n",
    "spending_by_specialty = spending_df.loc[:, ['specialty', 'spending']].groupby('specialty')\n",
    "\n",
    "spending_df['spending_pct'] = spending_by_specialty.transform(my_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_4-cf8sMd8Z"
   },
   "source": [
    "##### Transform Continued 2\n",
    "\n",
    "Suppose we wanted to see the percent spending by `drug` and `specialty`. One solution to achieve this would be to group on both the `specialty` and the `medication` columns and then sum the `spending_pct` that was computed previously.\n",
    "\n",
    "```python\n",
    ">>> medication_spending_pct =  spending_df.loc[:,['specialty', 'medication', 'spending_pct']].groupby([\"specialty\", \"medication\"]).sum()\n",
    ">>> medication_spending_pct.head(n=3)\n",
    "                                   spending_pct\n",
    "specialty  medication                        \n",
    "CARDIOLOGY ATORVASTATIN CALCIUM     50.791475\n",
    "           PANTOPRAZOLE SODIUM      29.960547\n",
    "           SIMVASTATIN              14.302612\n",
    "```\n",
    "\n",
    "Since we are grouping on two columns, the resulting index of `medication_spending_pct` also contains two columns. We need to reset (or drop) the index using the method `reset_index()` before we can sort on specialty and spending_pct.  `reset_index()` will set the index of the calling `DataFrame` to the default index, a range of integers.\n",
    "\n",
    "```python\n",
    ">>> medication_spending_pct = spending_df.loc[:, ['specialty', 'medication', 'spending_pct']].groupby([\"specialty\", \"medication\"]).sum().reset_index()\n",
    ">>> medication_spending_pct.sort_values([\"specialty\", \"spending_pct\"], ascending=[True, False]).head(n=4)\n",
    "       specialty                      medication  spending_pct\n",
    "0     CARDIOLOGY            ATORVASTATIN CALCIUM     50.791475\n",
    "1     CARDIOLOGY             PANTOPRAZOLE SODIUM     29.960547\n",
    "2     CARDIOLOGY                     SIMVASTATIN     14.302612\n",
    "3     CARDIOLOGY                 VENLAFAXINE HCL      4.945366\n",
    "```\n",
    "\n",
    "The resulting `DataFrame` is sorted by `specialty` first and then `spending_pct`. Each row tells us the percent of spending for each unique medication and specialty combination.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwPpV8mJNgrQ"
   },
   "outputs": [],
   "source": [
    "medication_spending_pct = spending_df.loc[:, ['specialty', 'medication', 'spending_pct']].groupby([\"specialty\", \"medication\"]).sum().reset_index()\n",
    "medication_spending_pct.sort_values([\"specialty\", \"spending_pct\"], ascending=[True, False]).head(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py-Gy0np6uq1"
   },
   "source": [
    "# Exercise 5.9: Transform\n",
    "\n",
    "Using the code cell below, \n",
    "* create a new `DataFrame` named `distance_and_day_df` that is a subset of `HNL_flights_df` containing only the `DAY` and `DISTANCE` columns. \n",
    "* Group `distance_and_day_df` by the `DAY` column and save the resulting `GroupBy` object in the variable `distance_by_day`. \n",
    "* Transfrom the `DISTANCE` column for each flight by calculating the percentage of the total distance by day the flight took. Save the result in a new column of  `HNL_flights_df` named `DISTANCE_PCT`. Use the function pre-defined in the cell to perform the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9B7K0AhEBMRF"
   },
   "outputs": [],
   "source": [
    "def percent_of_total(x):\n",
    "  return (x   / x.sum() ) * 100\n",
    "\n",
    "# Type your solution to Exercise 4.9 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKk2opbYyFHx"
   },
   "source": [
    "#### Filter\n",
    "\n",
    " __Filtering__  a group consists of dropping or retaining groups in a way that depends on a group-specific computation that returns `True` or `False`. Groups that are retained will be left unmodified. For instance, we can filter specialties from `spending_df` that don't have enough entries or for which the mean `spending` is below a certain threshold.\n",
    "\n",
    "Filtering a group is done using the `GroupBy` method `filter()`. The method `filter()` takes as input a function name, which it calls on each group of the `GroupBy` object. The function must return either `True` or `False` and groups for which the function returns `False` are dropped. The resulting `DataFrame` will have entries in the same order as the original `DataFrame`.\n",
    "\n",
    "Suppose we want to filter out the specialties that are low spending, i.e. we want to filter out the specialties for which the total spending is less than some defined threshold, let say $\\$50000$. To do this we can define a function named `filter_on_spending()`. The defined function will take a `DataFrame`, determine whether the sum total of the `spending` column in that `DataFrame` is greater than 50000, and then return `True` if it is or `False` if not. \n",
    "\n",
    "Then, to apply the filter on `spending_df`, we first subset the `DataFrame` so that only the columns `specialty` and `spending` are remaining and then group by `specialty`. Then the `GroupBy filter()` method can be called with the `filter_on_spending()` function passed as an argument. We can save the results into a new `DataFrame` named `high_spending_df`. Then to see which specialties surpassed the $\\$50000$ total spending threshold we can print the unique values in the `specialty` column of `high_spending_df`.\n",
    "\n",
    "```python\n",
    ">>> def filter_on_spending(x):\n",
    ">>>     return x['spending'].sum() > 50000\n",
    "\n",
    ">>> high_spending_df = spending_df[[\"specialty\", 'spending']].groupby('specialty').filter(filter_on_spending)\n",
    "\n",
    ">>> high_spending_df['specialty']\n",
    "array(['INTERNAL MEDICINE', 'ENDOCRINOLOGY'], dtype=object)\n",
    "```\n",
    "We see that only two specialties passed the threshold of total spending greater than $\\$50000$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8e9Y1zLSOaTT"
   },
   "outputs": [],
   "source": [
    "def filter_on_spending(x):\n",
    "     return x['spending'].sum() > 50000\n",
    "\n",
    "high_spending_df = spending_df[[\"specialty\", 'spending']].groupby('specialty').filter(filter_on_spending)\n",
    "\n",
    "high_spending_df['specialty'].unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39wMYAWQGsdI"
   },
   "source": [
    "# Exercise 5.10: Filter\n",
    "\n",
    "Using the code cell below, \n",
    "* Group `HNL_flights_df` by the `DAY` column and save the resulting `GroupBy` object in the variable `hnl_flights_by_day`. \n",
    "* Filter the flights by determining if the `ARRIVAL_DELAY` of the day was net positive, i.e. if there was a positive total delay for a day keep the flights, otherwise filter them out. Save the resulting `DataFrame` into the variable `HNL_flights_delayed_days_df`. Use the function pre-defined in the cell to perform the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFhrY6rlIs5g"
   },
   "outputs": [],
   "source": [
    "def net_postive_arrival_delay(x):\n",
    "  return  x.ARRIVAL_DELAY.sum() > 0\n",
    "\n",
    "# Type your solution to Exercise 4.10 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSrZmsDlyOWj"
   },
   "source": [
    "#### Thinning Data and The Flexible `apply()` Method\n",
    "\n",
    "`pandas` provides a few built-in `GroupBy` methods for thinning the data including `nlargest()`, `nsmallest()`, and more. An example usage of `nlargest()`, a thinning method, would be grouping a subset of `spending_df` which contains only the `spending` and `specialty` columns by `specialty` and then obtaining the 2 largest of each specialty. The result will be a new `DataFrame` with only the top 2 spenders from each unique specialty.\n",
    "\n",
    "```python\n",
    ">>> spending_by_specialty = spending_df.loc[:,['specialty', 'spending']].groupby('specialty')\n",
    ">>> spending_by_specialty['spending'].nlargest(2).head(n=4)\n",
    "specialty        unique_id\n",
    "CARDIOLOGY       CG865025      2726.72\n",
    "                 MK361461      1608.42\n",
    "ENDOCRINOLOGY    FV632964     76346.72\n",
    "FAMILY PRACTICE  FE564384     15640.59\n",
    "Name: spending, dtype: float64\n",
    "```\n",
    "    \n",
    "Some specialties only had a single representative which is why there are not two entries for 'ENDOCRINOLOGY'.\n",
    "\n",
    "Though `pandas` has the more common and basic aggregation, transformation, and thinning methods implmented for us, they could not possibly cover all cases. Therefore cases that do not fit into any one of these categories may be carried out by using the more flexible `apply() GroupBy` method. `apply()` takes as input a function name, which it calls on each group of the calling `GroupBy` object.\n",
    "\n",
    "For example suppose we wanted to thin our dataset so that there are only 50% of each specialty represented. To do this we can define a new function, we will call it, `sample_50p`, and this function will utilize the `sample()` `DataFrame` method. The `sample()` `DataFrame` will take a parameter `frac` that specifies the fraction of the original `DataFrame` that is to be returned. We can then use the `apply()` method and pass it the name of our newly defined function to obtain a new `DataFrame` that is filtered at the group specific level. \n",
    "\n",
    "```python\n",
    ">>> def sample_50p(x):\n",
    ">>>    return x.sample(frac=0.5)\n",
    "    \n",
    ">>> spending_by_specialty = spending_df.loc[:,['specialty', 'spending', 'medication']].groupby('specialty')\n",
    ">>> spending_by_specialty.apply(sample_50p).head(n=3)\n",
    "                                 specialty  spending            medication\n",
    "specialty       unique_id                                                 \n",
    "CARDIOLOGY      YS123432        CARDIOLOGY    265.49       VENLAFAXINE HCL\n",
    "                CG865025        CARDIOLOGY   2726.72  ATORVASTATIN CALCIUM\n",
    "FAMILY PRACTICE JX313970   FAMILY PRACTICE   8045.03           RISPERIDONE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzTzcosWUN0o"
   },
   "outputs": [],
   "source": [
    "def sample_50p(x):\n",
    "  return x.sample(frac=0.5)\n",
    "spending_by_specialty = spending_df.loc[:,['specialty', 'spending', 'medication']].groupby('specialty')\n",
    "spending_by_specialty.apply(sample_50p).head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1mmmDOqMUdTz"
   },
   "source": [
    "# Summary\n",
    "\n",
    "---\n",
    "\n",
    "**Reindexing**\n",
    "\n",
    "* Reindexing a `DataFrame` or `Series` will create a **new** `pandas` object that is conformed to the new index \n",
    "* More information about the `reindex()` method is available [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex.html)\n",
    "\n",
    "**Inspecting and Modifying Data Types**\n",
    "\n",
    "* Use the `DataFram dtypes` attribute to inspect the `pandas` data types of each column\n",
    "* To cast a column of one type to another compatible type we can use the `astype()` `Series` method\n",
    "\n",
    "**Series String Methods**\n",
    "\n",
    "* `Series` contains various `string` processing methods that can be accessed using a `Series`’s `str` property.\n",
    "* You can use .__`TAB`__ to explore these methods.\n",
    "* Or you can see all of the `Series` `str` methods and descriptions [here](https://pandas-docs.github.io/pandas-docs-travis/api.html#string-handling)\n",
    "\n",
    "**Handling Missing Data**\n",
    "\n",
    "* The  `isnull` method is often useful to find where all the `NaNs` precisely are in the `pandas` `DataFrame` or `Series`\n",
    "\n",
    "* **Filtering**\n",
    "\n",
    "  * You can discard missing values using `isnull()` result and subsetting\n",
    "\n",
    "* **Filling**\n",
    "\n",
    "  * There  are two conventional approaches for filling missing value:\n",
    "\n",
    "    1. Filling the value with a constant \n",
    "    2. Filling the value dynamically with something computed on the fly\n",
    "\n",
    "**Function Application and Mapping**\n",
    "\n",
    "* **Global Processing**\n",
    "\n",
    "  * To apply a function to every element in a `DataFrame` or `Series` we can use the `applymap()` method\n",
    "\n",
    "* **Group Specific Processing**\n",
    "\n",
    "  * The `groupby()` method is used to group the data using values on one or more columns\n",
    "\n",
    "  * `groupby()` is often applied in the context of the data processing paradigm called \"split-apply-combine\"\n",
    "    * **Split**: you need to split the data into chunks defined using one or more columns\n",
    "    * **Apply**: apply some operation on the chunks generated. \n",
    "    * **Combine**: combine the results of the applied operation into a new `DataFrame`\n",
    "\n",
    "  * There are 3 common classes of split-apply-combine operations that can be applied to group data.\n",
    "\n",
    "    1. __Aggregations__ generate a single value for each group\n",
    "  \n",
    "    2.  __Transformations__ convert the data and generate a group of the same size as the original group.\n",
    "\n",
    "    3.  __Filters__ retain or discard a group based on group-specific boolean computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9hLI3VPRq9u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_Data Preparation_and_Cleaning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
