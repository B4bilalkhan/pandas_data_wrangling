{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_Exploring_Data.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["0LJYevev1Ppg"],"toc_visible":true}},"cells":[{"metadata":{"id":"HOM84FzyZ8RG","colab_type":"text"},"cell_type":"markdown","source":["# Exploring Data\n","\n","\n","---\n","\n","It is crucial to have a deep understanding of your data in order to draw meaningful insights from it. In this chapter we will see how to use the built in functionalities of `pandas` to begin exploring our data. This will help us identify flaws in our dataset and hopefully inspire or even answer some interesting questions.\n","\n","For organizational purposes, we will begin, as always, with preparing our environment. Next, we move onto a review of the data set that we will be working with. Once we complete our preperation, we dive into learning new data exploration skills starting with attributes and methods for summarizing our data. After that, we continue with learning about arithmetic and data alignment between `Series` and `DataFrames` by covering vectorization and broadcasting. Then we will cover some more subsetting techniques before moving on to sorting. Finally we will close with some basics of data visualization using `matplotlib.pyplot`.\n"]},{"metadata":{"id":"_u2Qpd3q9jIq","colab_type":"text"},"cell_type":"markdown","source":["# Preparing our Environment\n","\n","---\n","\n","\n","Per usual, we begin with preparing our environement by loading the necessary libraries. In this chapter we will again be using the pandas toolkit with the standard alias `pd`, `pandas` has a variety of very useful built in functionality that will help us explore our datasets including basic plottting.\n","\n","```python\n","import pandas as pd\n","```\n","\n","`pandas` creates plots using the very popular `Matplotlib` data visualization package. `Matplotlib` was designed over 15 years ago to resemble `MATLAB` but still is the most powerful framework. `Matplotlib` has spawned many add-on toolkits for data visualization that build on top of `Matplotlib` and make it easier to generate publication ready plots, including `pyplot`,  `seaborn`, `holoviews`,  and `ggplot`; `pandas` uses `pyplot`. The data visualization package `matplotlib.pyplot` is commonly imported with the alias `plt`, a norm we will follow.\n","\n","```python\n","import matplotlib.pyplot as plt\n","```\n","\n","To be able see plots directly in `Jupyter`, we need to instruct `matplotlib` to show plots `inline`; i.e., below the code used to create them. This is done using the following command.\n","\n","```python\n","%matplotlib inline \n","```"]},{"metadata":{"id":"8qLC2OZbAfhe","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_Wc-H5aBEZY0","colab_type":"text"},"cell_type":"markdown","source":["# About the Data\n","\n","---\n","\n","Our goal in this chapter is to learn how to develop a deeper understanding of a dataset; we want to spot flaws in our data set, calculate some descriptive statistics, discover trends and patterns, etc. We will be using the same data set as previous chapters and by the end we will hopefully be more comfortable with its content and can begin forming some interesting questions. But before we begin, here is a brief review of what we are looking at:\n","\n","| Column |Description|\n","|:----------|-----------|\n","| `unique_id`| A unique identifier for a Medicare claim to CMS |\n","| `doctor_id` | The Unique Identifier of the doctor who <br/> prescribed the medicine  |\n","| `specialty` | The specialty of the doctor who prescribed the medicine |\n","| `medication` | The medication prescribed |\n","| `nb_beneficiaries` | The number of beneficiaries the <br/> medicine was prescribed to  |\n","| `spending` | The total cost of the medicine prescribed <br/>for the CMS |\n","\n","We will be working with the data stored in the file `'Data/spending_10k.csv'`, which is a subset of the complete dataset that is publicly available on the Centers for Medicare & Medicaid Services website ([`CMS` website](https://www.cms.gov/OpenPayments/Explore-the-Data/Dataset-Downloads.html)). This file has a header containing the labels of the column names so we will leave the `read_csv` `header` parameter to its default setting when we read the file using `pd.read_csv()`. We also would like to use the `unique_id` column as the index rather than a range of integers, so we will pass the name of the column, `unique_id`, to the `index_col` parameter of the `pd.read_csv()` function."]},{"metadata":{"id":"7M6A85qTIW1M","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_df = pd.read_csv('Data/spending_10k.csv', index_col='unique_id')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j3ggq3qsgUaE","colab_type":"text"},"cell_type":"markdown","source":["# Summarizing Your Data\n","\n","---\n","\n","It is often useful to quickly explore some of the descriptive attributes and statistics of the dataset that you are working with like the shape and datatypes of the `DataFrame`, and the range, mean, standard deviation, etc. of the rows or columns. You may find interesting patterns or possibly catch errors in your dataset this way. As we will see, accessing these attributes and computing the descriptive statistics is easy with `pandas`.  "]},{"metadata":{"id":"yfNSyBGVhAcC","colab_type":"text"},"cell_type":"markdown","source":["## Attributes\n","As was mentioned in Chapter 1: *Introduction to pandas Data Structures* , `DataFrames` have a number of attributes associated with them. With respect to exploring your dataset, perhaps the 3 most useful attributes are summarized in the table below:\n","\n","| Attribute |Description|\n","|:----------|-----------|\n","| `shape`| Return a tuple representing the dimensionality of the DataFrame. |\n","| `size` | Return an int representing the number of elements in this object.  |\n","| `dtypes` | Return the dtypes in the DataFrame. |\n","\n","In `Python`, you can access an object’s attribute using the syntax `ObjectName.attributeName`. For instance, if our `DataFrame` is named `df` and our attribute is `shape`, `df.shape`will return the shape attribute of our `DataFrame`.\n","\n","A list of all the `DataFrame` attributes can be found at the [`pandas` Documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)"]},{"metadata":{"id":"5un3yMEPj8pJ","colab_type":"text"},"cell_type":"markdown","source":["### Inspecting Data Types\n","\n","We have seen and have been using both `shape` and `size` and their relevance is clear, but it is not so obvious why we should be concerned with the data types of the `DataFrame`. One reason is that some methods can only work on specific data types.  For example, it would be unreasonable to compute the mean of the column `specialty` since the data type of `specialty` does not lend itself to being averaged.\n","\n","`DataFrames` store the additional `Series`, `dtypes`, which holds the data types in the `DataFrame` by column. The `dtypes` `Series` of a `DataFrame` allows `pandas` to instantly evaluate which methods can be applied to which columns. \n","\n","Let us look at the `dtypes` attribute of the `spending_df` `DataFrame`.\n","\n","```python\n",">>> spending_df.dtypes\n","doctor_id             int64\n","specialty            object\n","medication           object\n","nb_beneficiaries      int64\n","spending            float64\n","dtype: object\n","```\n","\n","We see that the `dtypes` attribute of the `spending_df` `DataFrame` is a `Series` with an `index` equivalent to the `columns` attribute of the `spending_df` `DataFrame`, and values which specify the data types of the entries in the corresponding column of the `DataFrame`."]},{"metadata":{"id":"ah77ttOkQHst","colab_type":"text"},"cell_type":"markdown","source":["![](images/dtypes.png) "]},{"metadata":{"id":"KyQuvWX2QEfj","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_df.dtypes"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tTtPTkaXjg7e","colab_type":"text"},"cell_type":"markdown","source":["# Exercise 3.0: Importing the Honolulu Flights Data Set\n","\n","For some of the exercises in this chapter we will be working with a data set containing information about all the arriving and departing flights in and out of the Honolulu aiport, HNL, on the Island of Oahu in December 2015. \n","\n","Please run the following code cell which will parse the 'honolulu_flights.csv' file, and build the `HNL_flights_df DataFrame` before trying the exercises in this chapter related to the Honolulu flights data set.\n","\n","This data set contains the following columns:\n","\n","| Column |Description|\n","|:----------|-----------|\n","| `YEAR` | The year of the flight  |\n","| `MONTH` |  The month of the flight |\n","| `DAY` |  The day of the flight |\n","| `DAY_OF_WEEK` |  The day of the week of the flight |\n","| `FLIGHT_NUMBER` |  The flight number of the flight |\n","| `ORIGIN_AIRPORT` |  The origin airport of the flight  |\n","| `DESTINATION_AIRPORT` |  The destination airport of the flight |\n","| `DEPARTURE_DELAY` |  The departure delay of the flight  |\n","| `DISTANCE` |  The distance of the flight in miles |\n","| `AIR_TIME` |  The flight time without taxiing in minutes |\n","| `ARRIVAL_DELAY` |  The arrival delay of the flight  |"]},{"metadata":{"id":"mVOqzGVdpbz1","colab_type":"code","colab":{}},"cell_type":"code","source":["HNL_flights_df = pd.read_csv('Data/honolulu_flights.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p8UmxUdzNe3f","colab_type":"text"},"cell_type":"markdown","source":["# Exercise 3.1: Attributes\n","\n","Which of the following lines of code will give the number of rows and columns, i.e. the shape attribute, of  the `HNL_flights_df DataFrame`?  Please note that the output should be: (7975, 11).\n","\n","A:\n","```python\n","HNL_flights_df.size\n","```\n","\n","B: \n","```python\n","HNL_flights_df.shape\n","```\n","\n","C: \n","```python\n","HNL_flights_df.shape()\n","```\n","\n","D: \n","```python\n","pd.shape(HNL_flights_df)\n","```\n","\n","Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown."]},{"metadata":{"id":"oTbr4EU2VhUS","colab_type":"code","colab":{}},"cell_type":"code","source":["# Exercise 3.1 Scratch Code Cell"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NpYXMxxvhTUy","colab_type":"text"},"cell_type":"markdown","source":["## Methods\n","\n","Please recall that in `Python` a method is a function that is accessible via an object instance, such as a `DataFrame`. The syntax for using a method is similar to accessing an attribute: `ObjectName.methodName()`. `DataFrames` have many built-in methods to summarize our data. The methods we will be most interested in to explore our data set are:\n","\n","| Method|Description|\n","|:----------|-----------|\n","| `head()`| Return the first n rows. |\n","| `tail()` | Return the last n rows. |\n","| `min()`, `max()` | Computes the numeric (for numeric value) or alphanumeric (for object values) row-wise min, max in a Series or DataFrame.|\n","| `sum()`, `mean()`, `std()`, `var()`   | Computes the row-wise sum, mean, standard deviation and variance in a `Series` or DataFrame.|\n","| `count()` |  Returns the number of non-NaN values in the in a `Series` or `DataFrame`. |\n","| `value_counts()` |  Returns the frequency for each value in the `Series`. |\n","| `describe()` | Computes row-wise statistics. |\n","\n","We will be covering when and how to use each of the methods described in the table above.\n","\n","If interested, a list and description of all the `DataFrame` methods may be found at the [pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)."]},{"metadata":{"id":"0t1p9XjxtSTl","colab_type":"text"},"cell_type":"markdown","source":["### Viewing Pieces of the Data\n","\n","When working with large `DataFrames` we may want to obtain a small sample of the `DataFrame` in order to get a quick overview of its organization (header, index, entries, etc....) without having to load it entirely. We had similar motivations in Chapter 2: *Loading and Storing data* when we introduced the `nrows` parameter of the `read_table()` and `read_csv()` `pandas` functions. \n","\n","Using the `head()` method we can make a new `DataFrame` with the same `columns` but only the ***first*** $n$ rows rather than the entire `DataFrame` where * $n$ is an optional parameter that is by default 5. For example, if we want to make a new `DataFrame` made of only the first 2 rows of `spending_df`, then we would type:\n","\n","```python\n",">>> spending_df.head(n=2)\n","              doctor_id   specialty       medication  nb_beneficiaries  spending\n","unique_id                                                                     \n","NX531425   1255626040  FAMILY PRACTICE  METFORMIN HCL                30   135.24 \n","QG879256   1699761833  FAMILY PRACTICE    ALLOPURINOL                30   715.76\n","```\n","\n","We see that the `DataFrame` method `head()` with the optional parameter `n` set to 2 returns a new `DataFrame` with only 2 rows but the same number of columns as the `spending_df` `DataFrame`.\n","\n","Similarly the `tail()` method we can make a new `DataFrame` with the same `columns` but only the ***last*** $n$ rows rather than the entire `DataFrame` where * $n$, again, is an optional parameter that is by default 5. "]},{"metadata":{"id":"p0OYeUMpRYQT","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_df.head(n=2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c_B4yGYN_cv-","colab_type":"text"},"cell_type":"markdown","source":["### DataFrame Axes\n","\n","We will begin to notice a concept of `axis` that is recurrent throughout the `pandas` `python` package. Many of the methods and functions we will see will have an optional `axis` parameter that may be set when the method of function is called. For instance, the methods `sum()`, `min()`, `max()`, etc.. can all be applied row- or column-wise. It helps to think about the operation as being carried across the axis.\n","\n","![](images/axis_example.png)\n","\n","The example seen in the image above is a visualization of how the `sum()` `DataFrame` method is carried out. The `sum()` method will calculate the sum of all the entries across either the rows or columns of the `DataFrame`, depending on how the optional `axis` parameter is set. Since the `axis` parameter of the `sum()` method is *optional*, it has a default setting, and in this case the default will be `axis=0` or equivalently `axis=rows`. However, this default may vary from method to method, so it is important to verify before use. \n","\n","When the `DataFrame` `sum()` method is called and the optional `axis` parameter is set to 0, or 'rows',  then the method will add up all the entries in the same column across all the rows, as is shown in the example above. \n","\n","We will see more examples of the `axis` parameter of `DataFrame` methods, and it will become more comfortable with practice. "]},{"metadata":{"id":"4RgUNUeEujno","colab_type":"text"},"cell_type":"markdown","source":["### Descriptive Statistics\n","\n","`pandas` has many methods to compute the most common descriptive statistics such as the minimum, maximum, mean, variance, etc. The collection of `DataFrame` methods which will calculate the descriptive statistics can operate on either axis (column or row). \n","\n","Let us see some examples of computing descriptive statistics. We will start by creating a very simple `DataFrame`, call it `df`, which will help us illustrate what is happening.\n","\n","```python\n",">>> df = pd.DataFrame({'A':[0,1,2], 'B':[5,6,7]})\n",">>> df\n","   A  B\n","0  0  5\n","1  1  6\n","2  2  7\n","```\n","\n","`df` is simply a $3 \\times 2$ `DataFrame` with numeric entries. If we wanted to calculate the mean of the all the row entries in each of the columns labeled `A` and `B`, then we could use the `mean()` `DataFrame` method with its default parameters, i.e. `axis='rows'`. \n","\n","``` python\n",">>> df.mean()\n","A    1.0\n","B    6.0\n","dtype: float64\n","```\n","\n","The resulting `Series` contains the mean of the columns `A` and `B`. Please note that the opeartion was carried across the \"rows\". \n","\n","Alternatively, if we wanted the mean of the column entries for each each row, then we would set the optional `axis` parameter of the `mean()` method to 1 or 'columns':\n","\n","```python\n",">>> df.mean(axis=1)\n","0    2.5\n","1    3.5\n","2    4.5\n","dtype: float64\n","```\n","\n","This time, we can see the mean of the rows. Also note that the operation is carried across the “columns”.\n","\n","Descriptive statistic methods can also be called by `Series` objects, i.e. individual rows and columns of the `DataFrame`. For example, if we only wanted to find the mean of all the entries in the column labeled by `A`, then we could first access the column `A` in `df` using the syntax: `df['A']`, and then call the `Series` `mean()` method on the returned `Series`. \n","\n","```python\n",">>> df['A'].mean()\n","1.0\n","```\n","\n","We see that the mean of all the entries in the column `A` is $1.0$. Note that the `Series` `mean()` method does *not* have the optional `axis` parameter since it would not make sense to calculate the mean across the 'columns' of a `Series` since `Series` do not have multiple columns. "]},{"metadata":{"id":"igP-4gtVXJSI","colab_type":"code","outputId":"88a8193b-1cd2-41a2-bab8-6a3b9f96a0f1","executionInfo":{"status":"ok","timestamp":1534190663139,"user_tz":600,"elapsed":445,"user":{"displayName":"Charles Dickens","photoUrl":"//lh6.googleusercontent.com/-LyFY-SmW2Wk/AAAAAAAAAAI/AAAAAAAAAJc/aBlebAB4Y_M/s50-c-k-no/photo.jpg","userId":"116926057727252856181"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"cell_type":"code","source":["df = pd.DataFrame({'A':[0,1,2], 'B':[5,6,7]})\n","df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>A</th>\n","      <th>B</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>7</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   A  B\n","0  0  5\n","1  1  6\n","2  2  7"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"f3dkBqdRX0nP","colab_type":"code","outputId":"2f1bc994-f3a4-41f0-db29-258b0e3a9d8d","executionInfo":{"status":"ok","timestamp":1534191029417,"user_tz":600,"elapsed":406,"user":{"displayName":"Charles Dickens","photoUrl":"//lh6.googleusercontent.com/-LyFY-SmW2Wk/AAAAAAAAAAI/AAAAAAAAAJc/aBlebAB4Y_M/s50-c-k-no/photo.jpg","userId":"116926057727252856181"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"cell_type":"code","source":["print('Column wise mean:')\n","print(df.mean())\n","print('---------------')\n","print('Row wise mean:')\n","print(df.mean(axis=1))\n","print('---------------')\n","print('Series mean:')\n","print(df['A'].mean())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Column wise mean:\n","A    1.0\n","B    6.0\n","dtype: float64\n","---------------\n","Row wise mean:\n","A    1.0\n","B    6.0\n","dtype: float64\n","---------------\n","Series mean:\n","1.0\n"],"name":"stdout"}]},{"metadata":{"id":"2gCbrD7AVQyd","colab_type":"text"},"cell_type":"markdown","source":["#### Descriptive Statistics on the Medical Spending Data Set\n","\n","Let us try and calculate some statistics for our Medical Spending Dataset, we will start with the mean\n","\n","```python\n",">>> spending_df.mean()\n","doctor_id           1.503766e+09\n","nb_beneficiaries    5.091830e+01\n","spending            4.333839e+03\n","dtype: float64\n","```\n","\n","Notice that the mean was calculated across the rows for all of the columns whose data type was numeric (int or float), and *not* for those whose data type was `object`. \n","\n","These results seem reasonable at first glance, but, if we look closely, it doesn't quite make sense to calculate the mean of the `doctor_id` column. We as humans can recognize that the `doctor_id` column is essentially just labels for each doctor that may have been assigned arbitrarily, so it would make more sense if the doctor IDs were instead stored as a `pandas` `object` type, since the `pandas` `object` type corresponds to the native `python` `string` data type. \n","\n","This is our first flaw that we shoud take note of in this dataset; we will learn how to clean this up in the following Chapter: *Data Cleaning*."]},{"metadata":{"id":"_UbhztYJapac","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_df.mean()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Cr-iw_-q08Qs","colab_type":"text"},"cell_type":"markdown","source":["### Count\n","\n","Earlier we introduced and used the `shape` and `size` attributes of `DataFrames` and `Series`. Recall that the `shape` and `size` attributes tell us how many rows and columns there are and how many entries there are in a `DataFrame` or `Series` respectively, but these attributes include missing values in their counts. To illustrate the point lets build a `DataFrame` with all missing values and check its shape and size:\n","\n","```python\n",">>> na_df = pd.DataFrame({1: [None, None], 2: [None, None]})\n",">>> na_df.shape\n","(2, 2)\n",">>> na_df.size\n","4\n","```\n","\n","If we were none the wiser then we would say that there are 2 rows and 2 columns with a total of 4 values in the `DataFrame`, but there is actually no real data in this `DataFrame`, all the entries are blank. \n","\n","Intstead of using `shape` and `size` to understand how many data points we have, we can instead use the built in `count()` method for `DataFrames` and `Series`, which will exclude missing values. In other words, `count()` tells us the number of non-missing values in a `DataFrame` or `Series`.\n","\n","Let us take a look at a practical example. First let us read the data in the file 'spending_missing_values.csv'. This file is specifcally constructed to have some missing values to demonstrate this concept on an interesting life like data set.\n","\n","```python\n",">>> spending_missing_values_df = pd.read_csv('spending_missing_values.csv', \n","                                               index_col='unique_id', \n","                                               na_values=['Null'])\n","```\n","\n","Now let us call the `count()` method of the `spending_missing_values_df` `DataFrame` to see how many missing values there are in each column. The method has the optional `axis` parameter with a default of `axis=0`, or equivalently 'axis=rows'.\n","\n","```python\n",">>> spending_missing_values_df.count()\n","doctor_id           54\n","specialty           48\n","medication          53\n","nb_beneficiaries    50\n","spending            50\n","dtype: int64\n","```\n","\n","The returned `Series` tells us the number of actual values in each `column` of the calling `DataFrame`. This `Series` shows us that there are many missing values in this `DataFrame`, and some columns have more missing values than others. \n","\n","If we wanted to know the total number of non-missing values in the `DataFrame` spending_missing_values_df, then we could call the `Series` `sum()` method on the returned `Series` from the `count()` call. For example, to find the total number of non-missing entries in the `DataFrame` `spending_missing_values_df` we would type:\n","\n","```python\n",">>> spending_missing_values_df.count().sum()\n","255\n","```\n","\n","The `sum()` `Series` method added up all the values in the `Series` returned by the `counts()` call, i.e. $54 + 48 + 53 + 50 + 50 = 255$."]},{"metadata":{"id":"NJZmcKQXdYy1","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_missing_values_df = pd.read_csv('Data/spending_missing_values.csv', \n","                                         index_col='unique_id', \n","                                         na_values=['Null'])\n","print(\"-------------Counts of Missing Values Across Rows----------------------\")\n","print(spending_missing_values_df.count())\n","print(\"---------------Total Count of Non-Missing Values-----------------------\")\n","print(spending_missing_values_df.count().sum())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nkBV89c8Zci3","colab_type":"text"},"cell_type":"markdown","source":["### Value Counts \n","\n","Not only will we be interested in counting how many non-missing values there are in a `DataSet`, but we may also be interested in counting the number of occurences of a unique value. `value_counts()` is strictly a `Series` method and does exactly that; `value_counts()` tells us the frequency of each unique value in the `Series`. \n","\n","For example, suppose we wanted to count the frequency of occurence of each value in the `specialty` column of the `spending_df` `DataFrame`. To do this we would first access the `specialty` column of the `spending_df` `DataFrame`,  then we would call the `value_counts()` method with the retrieved `Series`:\n","\n","```python\n",">>> spending_df['specialty'].value_counts()\n","INTERNAL MEDICINE                                                 3060\n","FAMILY PRACTICE                                                   2606\n","NURSE PRACTITIONER                                                 822\n",".\n",".\n",".\n","Name: specialty, Length: 75, dtype: int64\n","```\n","\n"," The values in the resulting `Series` correspond to the frequency of occurence of each `specialty`, for instance we see that the value 'INTERNAL MEDICINE' shows up in the `spending_df` `DataFrame` `specialty` column $3060$ times. Also notice that the length of the `Series` can be interepreted as the number of unique values in the `spending_df` `DataFrame` `specialty` column."]},{"metadata":{"id":"jOd7l47FdjEX","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_df['specialty'].value_counts().head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"33IbJxKK2xhW","colab_type":"text"},"cell_type":"markdown","source":["### The Describe Method\n","\n","If you just want a breif summary of your dataset including the main descriptive statistics and the counts then you can use the `describe()` method. By default, the `describe()` method will only describe the numeric columns, but we can set the `include` parameter to 'all' to see all of the columns described.\n","\n","First, let us look at the default description we are given when we call the `describe()` method for the `spending_df` `DataFrame`.\n","\n","```python\n",">>> spending_df.describe()\n","          doctor_id  nb_beneficiaries       spending\n","count  1.000000e+04      10000.000000   10000.000000\n","mean   1.503766e+09         50.918300    4333.838595\n","std    2.874269e+08         86.493443   21915.925814\n","min    1.003010e+09         11.000000      15.020000\n","25%    1.255580e+09         15.000000     253.237500\n","50%    1.508818e+09         24.000000     677.970000\n","75%    1.750467e+09         50.000000    2442.955000\n","max    1.992999e+09       1987.000000  892027.000000\n","```\n","\n","We see that the returned object is a `DataFrame` with column labels equivalent to the numeric columns of the calling `DataFrame` (`spending_df` in this case) and index labels of different descriptive statistics. For instance, the entry in the row labeled 'count' at the column labeled 'spending' tells us the count of non-missing values there are in the column `spending`of the `spending_df` `DataFrame`, and the 25%, 50% and 75% rows are the percentile values. This information is all very useful when you are trying to understand your data set.\n","\n","Now let us see what happens when we set the optional `include`parameter of the `describe()` `DataFrame` method to 'all', i.e. `include='all'`.\n","\n","```python\n",">>> spending_df.describe(include='all')\n","           doctor_id          specialty            medication  nb_beneficiaries       spending\n","count   1.000000e+04              10000                 10000      10000.000000   10000.000000\n","unique           NaN                 75                   617               NaN            NaN\n","top              NaN  INTERNAL MEDICINE  LEVOTHYROXINE SODIUM               NaN            NaN\n","freq             NaN               3060                   150               NaN            NaN\n","mean    1.503766e+09                NaN                   NaN         50.918300    4333.838595\n","std     2.874269e+08                NaN                   NaN         86.493443   21915.925814\n","min     1.003010e+09                NaN                   NaN         11.000000      15.020000\n","25%     1.255580e+09                NaN                   NaN         15.000000     253.237500\n","50%     1.508818e+09                NaN                   NaN         24.000000     677.970000\n","75%     1.750467e+09                NaN                   NaN         50.000000    2442.955000\n","max     1.992999e+09                NaN                   NaN       1987.000000  892027.000000\n","```\n","\n","We now see that the returned `DataFrame` from the `describe()` call includes all of the columns of the `spending_df` `DataFrame`. Also notice that there are additional rows labeled 'top' and 'freq'. The 'top' and 'freq' row entries tell us the most common entry in the column and the corresponding frequency of that top entry respectively. \n","\n","The 'top' and 'freq' row entries are NaN, i.e. missing, for the numeric columns. This is because it doesn't quite make sense to calculate those statistics for the numeric columns. Similarly the count, mean, std, etc. row entries are NaN in the object type columns, since we cannot calculate these statistics in the object columns."]},{"metadata":{"id":"47v6pq8-eu-m","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_df.describe()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bAoAoPNXexkT","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_df.describe(include='all')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"spCbYjeY6Afg","colab_type":"text"},"cell_type":"markdown","source":["# Exercise 3.2: `DataFrame` Methods\n","\n","Which of the following lines of code will output the average arrival delay time for the flights described in `HNL_flights_df`? Note that the output should be: -2.2572254335260116\n","\n","A:\n","```python\n","HNL_flights_df['ARRIVAL_DELAY'].mean()\n","```\n","\n","B:\n","```python\n","HNL_flights_df.mean()\n","```\n","\n","C:\n","```python\n","pd.mean(HNL_flights_df.ARRIVAL_DELAY)\n","```\n","\n","D:\n","```python\n","HNL_flights_df.describe().ARRIVAL_DELAY\n","```\n","\n","Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown."]},{"metadata":{"id":"tQlBznsp9l49","colab_type":"code","colab":{}},"cell_type":"code","source":["# Exercise 3.2 Scratch Code Cell"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qcm-BNiPgSsM","colab_type":"text"},"cell_type":"markdown","source":["# Arithmetic and Data Alignment\n","\n","---"]},{"metadata":{"id":"Ft4T8eRUiJW9","colab_type":"text"},"cell_type":"markdown","source":["## Vectorization\n","\n","***Vectorization*** is simply operations applied index by index to array like data structures. Vectorization is important since it is a much more efficient alternative to for loops in `Python`. \n","\n","`pandas` seamlessly supports vectorization. A key feature of pandas `Series` and `DataFrames` is that when executing an arithmetic operation, the `Series` or `DataFrames` will first be aligned by their matching indices and applied in a pairwise fashion. A new index is created from the union of the indices of both `Series` or `DataFrames` and values for indices present in only one of the `Series` or `DataFrames` are filled with missing values (NaN). \n","\n","In the following cells we will be seeing examples of how to perform vectorized operations on `Series` and `DataFrames`. To demonstrate the process without getting lost in the complexities of a life like dataset we will be working with two very basic `DataFrames`, call them `df_1` and `df_2`, that we will construct like so:\n","\n","```python\n","df_1 = pd.DataFrame({'AA':{'A':79, 'C':2, 'T':12, 'X':21},\n","                     'BB':{'A':11, 'C':2, 'T':2, 'X':9}})\n","df_2 = pd.DataFrame({'AA':{'A':21,'D':14,'T':5},\n","                     'CC':{'A':12,'D':28,'T':121}})\n","```\n"]},{"metadata":{"id":"nPMiIVzJy7cc","colab_type":"code","colab":{}},"cell_type":"code","source":["df_1 = pd.DataFrame({'AA':{'A':79, 'C':2, 'T':12, 'X':21},\n","                     'BB':{'A':11, 'C':2, 'T':2, 'X':9}})\n","df_2 = pd.DataFrame({'AA':{'A':21,'D':14,'T':5},\n","                     'CC':{'A':12,'D':28,'T':121}})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YfChLH62O48W","colab_type":"text"},"cell_type":"markdown","source":["### Vectorized Arithmetic Between `Series` (`DataFrame` col)\n","\n","Let us first look at an example of performing arithmetic between `pandas` `Series`. If we wanted to add the column labeled 'AA' in `df_1` to the column labeled \"AA\" in `df_2` we would type:\n","\n","```python\n","df_1[\"AA\"] + df_2[\"AA\"]\n","```\n","\n","![](images/alignment_arithmetic_col.png)\n","\n","The image above breaks down the process of adding the `Series`. First we see that the two columns, both labeled 'AA' are accessed from the `DataFrame` `df_1` and `df_2` by typing `df_1['AA']` and `df_2['AA']` respectively. Second, the two column `Series` are aligned by the their individual indices. Notice in the image that the `Series` `df_1['AA']` was extended to include the new entry labled 'D' and the new row entry was populated by `NaN`. Similarly the `Series` `df_2['AA']` was extended to include two new entries with labels 'C' and 'X' and they too were populated with `NaNs`. Lastly, the two extended `Series` are added together to make a new `Series` with an index equivalent to the union of the indices of both of the two `Series` `df_1['AA']` and `df_2['AA']`. Each entry of the new `Series` is the sum of the entries with the same labels in `df_1['AA']` and `df_2['AA']`, but notice that if either one of the entries in the `Series` was `NaN`, then the result was also `NaN`. "]},{"metadata":{"id":"bAmUUSvF2xRh","colab_type":"code","colab":{}},"cell_type":"code","source":["df_1[\"AA\"] + df_2[\"AA\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ErZIn1TSP5KI","colab_type":"text"},"cell_type":"markdown","source":["### Vectorized Arithmetic Between `Series` (`DataFrame` Row)\n","\n","The process behind adding row `Series` is very similar to the process of adding column `Series`, the only difference is that we initially acess the rows of the two `DataFrames`. For instance, to add the row `Series` `df_1.loc[\"A\", :]` to the row `Series`  `df_2.loc[\"D\", :]` we would type:\n","\n","```python\n","df_1.loc[\"A\"] + df_2.loc[\"D\"]\n","```\n","\n","![](images/alignment_arithmetic_row.png)\n","\n","\n","The image above again breaks down the process of what is happening when we run this small command."]},{"metadata":{"id":"41SxTmdV5wvU","colab_type":"code","colab":{}},"cell_type":"code","source":["df_1.loc[\"A\"] + df_2.loc[\"D\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PIG2G5gkPADE","colab_type":"text"},"cell_type":"markdown","source":["### Vectorized Arithmetic Between DataFrames\n","\n","Our final example is arithmetic between two entire `DataFrames`. The concept behind what is happening here is just an extension of both adding row and column `Series`; when we add two `DataFrames` in a vectorized way, the alignment is with both the row and column index. For example let us add the two `DataFrames` `df_1` and df_2`.\n","\n","```python\n","df_1 + df_2\n","```\n","\n","![](images/alignment_arithmetic_df.png)\n","\n","The image above breaks down the two step process of adding the `DataFrames`. First the `DataFrames` are extended so that they have matching column labels and indices. Then the `DataFrames` are added entry by entry, meaning the entries with the same index and column label are added, to create a new `DataFrame`. Remember that if an entry that is missing is added to a another entry, then the missing value, `NaN`, will carry through to the new `DataFrame`."]},{"metadata":{"id":"oUlho1G3ne1P","colab_type":"code","outputId":"643ae759-cf1d-4491-9e28-b60bd60f3cad","executionInfo":{"status":"ok","timestamp":1534192063202,"user_tz":600,"elapsed":408,"user":{"displayName":"Charles Dickens","photoUrl":"//lh6.googleusercontent.com/-LyFY-SmW2Wk/AAAAAAAAAAI/AAAAAAAAAJc/aBlebAB4Y_M/s50-c-k-no/photo.jpg","userId":"116926057727252856181"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"cell_type":"code","source":["df_1 + df_2"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>AA</th>\n","      <th>BB</th>\n","      <th>CC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>A</th>\n","      <td>100.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>C</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>D</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>T</th>\n","      <td>17.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>X</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      AA  BB  CC\n","A  100.0 NaN NaN\n","C    NaN NaN NaN\n","D    NaN NaN NaN\n","T   17.0 NaN NaN\n","X    NaN NaN NaN"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"CKrdPyVogezm","colab_type":"text"},"cell_type":"markdown","source":["## Vectorization Example with the Medical Spending Data Set\n","\n","We have seen vectorization with basic `DataFrames`, now at a more concrete example . What if we wanted to calculate the average spending per beneficiary of the `spending_df` `DataFrame`?\n","\n","To do this we could first make a new `Series` that is the spending per benificiary by dividing `spending` column by the 'nb_beneficiaries' column. Then the average of this new `Series` can be calculated by calling the `mean()` method.  This can all be done in one line by chaining the operations together.\n","\n","```python\n",">>>> (spending_df['spending'] / spending_df['nb_beneficiaries']).mean()\n","131.92616419345254\n","```"]},{"metadata":{"id":"O9yZ2OYGftw6","colab_type":"code","colab":{}},"cell_type":"code","source":["(spending_df['spending'] / spending_df['nb_beneficiaries']).mean()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NVxb8RGe3N-a","colab_type":"text"},"cell_type":"markdown","source":["# Exercise 3.3: Vectorization\n","\n","Which of the following lines of code will result in a `Series` that contains the average speed of the plane in miles per minute for each flight decribed in  HNL_flights_df? Please note that this can be calculated by dividing the distance by the air time of the flight.\n","\n","A:\n","```python\n","HNL_flights_df.loc[:, 'DISTANCE' / 'AIR_TIME']\n","```\n","\n","B:\n","```python\n","HNL_flights_df.loc['DISTANCE', :] / HNL_flights_df.loc['AIR_TIME', :]\n","```\n","\n","C:\n","```python\n","HNL_flights_df.loc[:, 'DISTANCE'] / HNL_flights_df.loc[:, 'AIR_TIME']\n","```\n","\n","D:\n","```python\n","HNL_flights_df.loc[:, ['DISTANCE', 'AIR_TIME']].divide(HNL_flights_df.loc[:, 'AIR_TIME'], axis='rows')\n","```\n","\n","Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown."]},{"metadata":{"id":"3CEGz95h8VEs","colab_type":"code","colab":{}},"cell_type":"code","source":["# Exercise 3.3 Scratch Code Cell"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r5pyeKdsiMyT","colab_type":"text"},"cell_type":"markdown","source":["## Broadcasting\n","\n","Arithmetic operations between `Series` or `DataFrames` and a scalar require expanding the scalar into a `Series` or `DataFrame` of the same dimension. This process of expanding a single value into a `Series` or `DataFrame` is called **broadcasting**. \n","\n","For example, suppose we wanted to add the value 1.2 to `Series` `df_1['AA']`, where `df_1` is the same `DataFrame` we were using in the section on *Vectorization*.  To do this, all we would need to type is the following:\n","\n","```python\n","df_1['AA'] + 1.2\n","```\n","\n","![](images/alignment.png)\n","\n","\n","The image above shows what is happening when we type this command. We see that a new `Series` is created with an index matching the index of `df_1['AA']` and with entries of all the same value, 1.2. This new `Series` is aligned with `df_1['AA']` and added entry by entry, just like in the *Vectorization* examples. "]},{"metadata":{"id":"UYL_NlZmnmFu","colab_type":"code","colab":{}},"cell_type":"code","source":["df_1['AA'] + 1.2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YSOZ-BzsDShV","colab_type":"text"},"cell_type":"markdown","source":["# Exercise 3.4: Broadcasting\n","\n","Currently the `AIR_TIME` column, which gives the total flight time without taxiing, is in units of minutes. Which of the following lines of code will result in a `Series` that contains the total flight time without taxiing in units of hours? Please note that this can be found by dividing each of the entries in the `AIR_TIME` column by sixty.\n","\n","A:\n","```python\n","HNL_flights_df.loc[:, 'AIR_TIME'] / pd.DataFrame([60])\n","```\n","\n","B:\n","```python\n","HNL_flights_df.loc[:, 'AIR_TIME'] / pd.Series([60])\n","```\n","\n","C:\n","```python\n","HNL_flights_df.loc['AIR_TIME', :] / 60\n","```\n","\n","D:\n","```python\n","HNL_flights_df.loc[:, 'AIR_TIME'] / 60\n","```\n","\n","Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown."]},{"metadata":{"id":"RF8mhDhAGyyH","colab_type":"code","colab":{}},"cell_type":"code","source":["# Exercise 3.4 Scratch Code Cell"],"execution_count":0,"outputs":[]},{"metadata":{"id":"scMKa-1MgEWn","colab_type":"text"},"cell_type":"markdown","source":["# Subsetting\n","\n","---\n","\n","When exploring our data we will often want to focus our attention to specific rows, columns, and entries that satisfy a certian condition, i.e. we want to work with a subset of the orignal data. \n","\n","We briefly mentioned subsetting in the $1^{st}$ chapter: *Introduction to pandas Data Structures* .There we were indexing rows and columns and we saw that we could retrieve multiple Rows and Columns at once by passing a list of *labels* or *Integer-locations*. We also saw that, as with Python `lists`, subsetting for both `DataFrames` and `Series` can be carried out with the **range** operator \":\".\n"," \n"," Now we will see how we could subset our `Data` using **Comparison** and **Boolean** operations by creating boolean (True and False) `DataFrames` and `Series`"]},{"metadata":{"id":"dCjOGvOmh7OG","colab_type":"text"},"cell_type":"markdown","source":["## Comparison Operations\n","\n","Comparison Operations (\"<\" , \">\" , \"==\" , \">=\" , \"<=\" , \"<>\" , \"!=\") are applied to pandas `Series` and `DataFrames` in the same vectorized fashion as arithmetic operations except the returned object is a `Series` or `DataFrame` of Booleans (either True or False).\n","\n","To demonstrate the concepts needed to subset `DataFrames` let us create a new `DataFrame` `df_3` to use along with our existing `df_1` and `df_2` `DataFrames`.\n","\n","```python\n",">>> df_3 = pd.DataFrame({'AA':{'A':64, 'C':2, 'T':6, 'X':22}, \n","                           'BB':{'A':3, 'C':2, 'T':7, 'X':12}})\n","```\n","\n","Now let us examine what happens when we use the \">\" operator between `df_1.loc[:, 'AA']` and `df_3.loc[:, 'AA']`.\n","\n","```python\n",">>> df_1.loc[:, \"AA\"] > df_3.loc[:, \"AA\"]\n","A     True\n","C    False\n","T     True\n","X    False\n","Name: AA, dtype: bool\n","```\n","\n","The result of this operation is a new `Series` with the same indices as both `df_1` and `df_2` and with all entries of type `bool`.  The entries at a given index tell us whether the value at the same index in `df_1[\"AA\"]` is greater than the value at the same index in `df_3[\"AA\"]`. For instance, we can see that the entry at index 'A' in the `Series` `df_1[AA]` is indeed greater than the entry at label 'A' in `df_3['AA']`.\n","\n","Now for an important note: comparisons between 2 `Series` or `DataFrames` can only be done if both objects are identically labeled or we will get the following error:\n","\n","```python\n",">>> df_1.loc[:, \"AA\"] > df_2.loc[:, \"AA\"]\n","...\n","ValueError: Can only compare identically-labeled Series objects\n","```\n","\n","This is why we created the new `df_3` data frame as we did, so that its indices matched that of `df_1`. "]},{"metadata":{"id":"EDN5mfqQn19O","colab_type":"code","colab":{}},"cell_type":"code","source":["df_3 = pd.DataFrame({'AA':{'A':64, 'C':2, 'T':6, 'X':22},\n","                     'BB':{'A':3, 'C':2, 'T':7, 'X':12}})\n","\n","df_1[\"AA\"] > df_3[\"AA\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E-MJyvA0V_kc","colab_type":"text"},"cell_type":"markdown","source":["### Comparison Operations  and Indexing\n","\n","Comparison operators are ideal for querying and subsetting . We can subset a `Series` using another equally sized list (or a `Series`) of `Boolean`s.\n","\n","Let us take a look at the following example. We will subset the `Series` `df_1` using a manually constructed list of `Booleans`, this list is carefully made so that it is the same size as the `Series`. They have to have the same number of entries, otherwise this will not work.  \n","\n","```python\n",">>> df_1.loc[:, 'AA'][[True, True, False, False]]\n","A    79\n","C     2\n","Name: AA, dtype: int64\n","```\n","\n","We see in the above example that a new `Series` is returned with only the entries positionally aligned with the True values in the list that we passed.\n","\n","We can also subset `DataFrames` using other equally shaped `DataFrame` of `Boolean`s. To show this we will subset the `DataFrame` df_1 by creating a new `Boolean` `DataFrame` of the same size that was created using a comparison operator, specifically `df_1 > df_3`\n","\n","```python\n",">>> df_1[df_1 > df_3]\n","       AA    BB\n","A  79.0  11.0\n","C   NaN   NaN\n","T  12.0   NaN\n","X   NaN   NaN\n","```\n","\n","We see that a `DataFrame` is returned where the entries aligned with the `False` values are replaced with `NaN`s. The only values not missing are those that were larger than their corresponding entries in `df_3`. \n","\n","Lastly, we will show that a `DataFrame`  can also be subsetted with a `Boolean` `Series` or `list`, and we will see a slightly different result than when we subsetted using a `DataFrame`. Suppose we want a subset of the `DataFrame` `df_1` with only the rows where the column entry of the row at 'AA' is greater than the entry in the same position in the `DataFrame` `df_2`. We could do this using the following command:\n","\n","```python\n",">>>> df_1.loc[df_1.loc[:, 'AA'] > df_3.loc[:, 'AA'], :]\n","     AA  BB\n","A  79  11\n","T  12   2\n","```\n","\n","In the example above we can first look at what is inside the square brackets, `[df_1['AA'] > df_3['AA']]`. This comparison operator will result in a `Boolean` `Series` where the entries tell us whether or not the entry in `df_1['AA']` was greater than in `df_3['AA']`, this example was shown in the section above on comparison operations. The new `Series` is then used to subset the `DataFrame` `df_1` by rows, i.e. the row in position $i$ will exist in the new subsetted `DataFrame` if the `Series` entry at $i$ is `True`.\n","\n","The same can be done on the `row` dimension as shown in the example below:\n","```python\n",">>> df_1.loc[:, df_1.loc[:, 'T'] > df_3.loc[:, 'T']]\n","     BB\n","A  11\n","C    2\n","T    2\n","X    9 \n","```\n","\n"]},{"metadata":{"id":"X71kcE8ZpMWx","colab_type":"code","colab":{}},"cell_type":"code","source":["df_1.loc[:, 'AA'][[True, True, False, False]]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fLRtQ9WTp1Cw","colab_type":"code","colab":{}},"cell_type":"code","source":["df_1[df_1 > df_3]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2xkdYowmqowg","colab_type":"code","colab":{}},"cell_type":"code","source":["df_1.loc[df_1.loc[:, 'AA'] > df_3.loc[:, 'AA'], :]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jvm0Oazenvco","colab_type":"code","colab":{}},"cell_type":"code","source":["df_1.loc[:,df_1.loc['T'] > df_3.loc['T']]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6l9UTGRjZV5c","colab_type":"text"},"cell_type":"markdown","source":["### Subsetting With Booleans Example\n","\n","Let us take a moment to look at a simple example and breakdown subsetting a `DataFrame` since it is an important concept that will be used often. Let us subset the `DataFrame` `df_1` using a manually constructed `Boolean` list.\n","\n","```python\n",">>> df_1.loc[[True, False, True, False], :] \n","```\n","\n","![](images/filter_dataframe.png )\n","\n","The image above shows the procedure of subsetting a `DataFrame` with a `Boolean` list or `Series`. The `Boolean` list is first aligned by position with the `DataFrame` rows. Every row that aligns with a True value is kept and every row that aligns with a False value is removed, leaving a subset of the original `DataFrame`."]},{"metadata":{"id":"sFHCbFRCMGa1","colab_type":"text"},"cell_type":"markdown","source":["# Exercise 3.5: Subsetting with Comparison Operations\n","\n","\n","Suppose we want to focus our attention to flights that had arrival delays. Which line of code will correctly return a subset of `HNL_flights_df` containg only flights that had a positive arrival delay?\n","\n","\n","A:\n","```python\n","HNL_flights_df.loc[:, 'ARRIVAL_DELAY'] > 0\n","```\n","\n","B:\n","```python\n","HNL_flights_df[HNL_flights_df.loc[:, 'ARRIVAL_DELAY'] > 0]\n","```\n","\n","C:\n","```python\n","HNL_flights_df.loc[:, 'ARRIVAL_DELAY' > 0]\n","```\n","\n","D:\n","```python\n","HNL_flights_df[HNL_flights_df > 0]\n","```\n","\n","Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown."]},{"metadata":{"id":"54yK892IMQHA","colab_type":"code","colab":{}},"cell_type":"code","source":["# Exercise 3.5 Scratch Code Cell"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3YRBdeK5h--o","colab_type":"text"},"cell_type":"markdown","source":["## Boolean Operations\n","\n","We can compose conditional expressions to create more sophisticated queries for our dataset. To do this we use `Boolean` operations. The `Boolean` operations that native Python uses are slightly different than what pandas will recognize. Python's \"`and`\" is replaced with \"`&`\", \" `or` \" is replaced with \"`|`\" , and \"`not`\"  is replaced with \"`~`\"\n","\n","Let us find the rows of the toy `DataFrame` `df_1`, where both the entries in the `AA` and `BB` columns are greater than those in `df_3`\n","\n","```python\n",">>> true_false_series = (df_1['AA'] > df_3['AA']) & (df_1['BB'] > df_3['BB']) \n",">>> df_1[true_false_series]\n","   AA  BB\n","A  79  11\n","```\n","\n","In the example above we first created two `Boolean` `Series`, one that identifies where `df_1['AA']` is greater than `df_3['AA']`, and the other identifies where `df_1['BB']` is greater than `df_3['BB']`. These two `Series` are then \"anded\" together to make a new `Boolean` `Series` using the logical `&` operator. This `&` operator will check whether the entries in the same positions of the two `Series` `(df_1['AA'] > df_3['AA'])` and `(df_1['BB'] > df_3['BB'])` are both `True`, and if so the entry in the new `Series` will also be `True`, otherwise it will be `False`.  This new `Boolean` `Series` identifies where both the entries in the `AA` and `BB` columns in `df_1`are greater than those in `df_3`. Lastly we use the new `Series` to subset the `DataFrame` `df_1`. The resulting subset of `df_1` is the `DataFrame` with rows where both the entries in the `AA` and `BB` columns of `df_1` are greater than those in `df_3`."]},{"metadata":{"id":"pjLeIIdgvX3e","colab_type":"code","colab":{}},"cell_type":"code","source":["true_false_series = (df_1['AA'] > df_3['AA']) & (df_1['BB'] > df_3['BB']) \n","df_1[true_false_series]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3HzjmNVfuWXi","colab_type":"text"},"cell_type":"markdown","source":["### Boolean Operations Examples with the Medical Spending Data Set\n","\n","We saw subsetting with basic `DataFrames`, now lets look at our `spending_df` `DataFrame`, something more similar to what you might see in practice.\n","\n","Suppose we wanted to find the rows where the `specialty` was 'Dentist' and the spending was below \\$100 and save the results to a new `DataFrame`. To do this we could first make a `boolean` `Series` that will index our `DataFrame` as desired by composing the conditional statements: `(spending_df['specialty'] == 'DENTIST')` and ` (spending_df['spending'] < 100)` with the `&` operator. Then we could subset the original `DataFrame` using our `boolean` `Series` and save the result into a new `DataFrame`, call it `small_spending_dentist_df`. \n","\n","```python\n",">>> dentist_and_small_spender = (spending_df.loc[:, 'specialty'] == 'DENTIST') & (spending_df.loc[:, 'spending'] < 100)\n",">>> small_spending_dentist_df = spending_df.loc[dentist_and_small_spender,  :]\n",">>> small_spending_dentist_df.head(n=2)\n","              doctor_id specialty medication  nb_beneficiaries  spending\n","unique_id                                                             \n","XY759578   1114930567   DENTIST  IBUPROFEN                23     67.19\n","NR408938   1407936230   DENTIST  IBUPROFEN                20     64.20\n","```"]},{"metadata":{"id":"Ucfq9wi9UbVs","colab_type":"code","colab":{}},"cell_type":"code","source":["dentist_and_small_spender = (spending_df.loc[:, 'specialty'] == 'DENTIST') & (spending_df.loc[: 'spending'] < 100)\n","small_spending_dentist_df = spending_df.loc[dentist_and_small_spender, :]\n","small_spending_dentist_df.head(n=2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yKdbLqM3P-6p","colab_type":"text"},"cell_type":"markdown","source":["# Exercise 3.6: Subsetting with Boolean Operations\n","\n","Suppose we wanted to focus on flights that had both departure and arrival delays. Which line of will correctly return a subset of `HNL_flights_df` containing only flights that had both departure and arrival delays?\n","\n","A:\n","```python\n","HNL_flights_df[(HNL_flights_df.loc[:, 'ARRIVAL_DELAY'] > 0) | (HNL_flights_df.loc[:, 'DEPARTURE_DELAY'] > 0)]\n","```\n","\n","B:\n","```python\n","HNL_flights_df[HNL_flights_df.loc[:, 'ARRIVAL_DELAY'] > 0 & HNL_flights_df.loc[:, 'DEPARTURE_DELAY'] > 0]\n","```\n","\n","C:\n","```python\n","HNL_flights_df[(HNL_flights_df.loc[:, 'ARRIVAL_DELAY'] > 0) && (HNL_flights_df.loc[:, 'DEPARTURE_DELAY'] > 0)]\n","```\n","\n","D:\n","```python\n","HNL_flights_df[(HNL_flights_df.loc[:, 'ARRIVAL_DELAY'] > 0) & (HNL_flights_df.loc[:, 'DEPARTURE_DELAY'] > 0)]\n","```\n","\n","Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown."]},{"metadata":{"id":"WxlJzRRFQEOW","colab_type":"code","colab":{}},"cell_type":"code","source":["# Exercise 3.6 Scratch Code Cell"],"execution_count":0,"outputs":[]},{"metadata":{"id":"45tZYcTLidqs","colab_type":"text"},"cell_type":"markdown","source":["# Sorting\n","\n","---\n","\n","We may want to see the data sorted by some criterion to explore potential orderings of the entries. Consider `spending_df`,  we could order this `DataFrame` by spending amount from high to low, or from low to high. In the following sections we will learn how to do just that."]},{"metadata":{"id":"VE6tdG5xjgf5","colab_type":"text"},"cell_type":"markdown","source":["## Sorting by Index\n","\n","We can easily sort a `DataFrame` lexicographically by either its row or column index using the built-in pandas `DataFrame` `sort_index()` method. We may specify whether we want to sort using the row or column index using the familiar `axis` parameter. Additionally, we may specify whether we want to sort in descending order by setting the `ascending` parameter to `False`.  \n","\n","For example let us sort the `index` of the `spending_df` `DataFrame`.\n","\n","```python\n",">>> spending_df.sort_index().head(n=2)\n","            doctor_id          specialty                    medication  ...\n","unique_id                                                                \n","AA137628   1619994506  INTERNAL MEDICINE            PRAVASTATIN SODIUM   \n","AA150698   1124243563         CARDIOLOGY  FENOFIBRATE NANOCRYSTALLIZED \n","```\n","In this example we see that a `DataFrame` is returned so that the original is not altered, i.e. a copy was made. We also chained methods when we called the `head()` method on the returned `DataFrame`, this is so that we do not attempt to print all 10k entries in the data set. We see that the first two indices are AA137628 and AA150698, which tells us the sort was successful since those two entries are in lexicographical order and they seem to be two entries that would be high on the order since they start with AA.\n","\n","More information about the `sort_index` method may be found at the [pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_index.html)."]},{"metadata":{"id":"oKFniAdQUS7P","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_df.sort_index().head(n=2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Vra9sOJpjpX-","colab_type":"text"},"cell_type":"markdown","source":["## Sorting by Value\n","\n","We may also wish to sort a `DataFrame` by the values in one of its columns. To do this we use the pandas `DataFrame` `sort_values()` method. The `sort_values()` method has a required parameter, `by`, that needs to be provided the name(s) of the column(s) we wish to sort the `DataFrame` by.\n","\n","For example suppose we wanted to create a new `DataFrame` that is sorted in descending order by the values in the column 'sorting'. To do this we would type:\n","\n","```python\n","spending_df.sort_values(by='spending', ascending=False).head(n=2).loc[:, ['specialty', 'medication', 'spending']]\n","                     specialty             medication   spending\n","unique_id                                                     \n","RU995312   PULMONARY DISEASE            AMBRISENTAN  892027.00\n","SJ341179    GASTROENTEROLOGY  LEDIPASVIR/SOFOSBUVIR  855440.65\n","```\n","\n","We first sorted the `DataFrame` by the values in the `spending` column to create a new `DataFrame` using the `sort_values()` `DataFrame` method with `by='spending'` and ascending=False. Then we called the `head` method on the new `DataFrame` to retrieve the first 2 rows. Lastly, we indexed the `DataFrame` so that only the index and the 3 columns: `specialty`, `medication`, and `spending` are displayed. The result is as desired, a new `DataFrame` sorted in descending order by the values in the `spending` column, i.e. the top 'spending' rows are first in the `DataFrame`. "]},{"metadata":{"id":"S3mOciAtcwPh","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_df.sort_values('spending', ascending=False).head(n=2).loc[:,\n","    ['specialty', 'medication', 'spending']]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HqoDAgUuNBfp","colab_type":"text"},"cell_type":"markdown","source":["# Exercise 3.7: Sorting\n","\n","Which line of code will correctly sort `HNL_flights_df` by the arrival delay in order of greates to least, i.e. in descending order?\n","\n","A:\n","```python\n","HNL_flights_df.sort_values(by='ARRIVAL_DELAY', ascending = False)\n","```\n","\n","B:\n","```python\n","HNL_flights_df.sort_values(by='ARRIVAL_DELAY', ascending = True))\n","```\n","\n","C:\n","```python\n","HNL_flights_df.sort_index(by='ARRIVAL_DELAY')\n","```\n","\n","D:\n","```python\n","HNL_flights_df.sort(by='ARRIVAL_DELAY', ascending = False)\n","```\n","\n","Hint: Feel free to use the code cell below to try these commands out. For the incorrect options, make note of what is going wrong and or what errors are being thrown."]},{"metadata":{"id":"A9ubD0iKPL76","colab_type":"code","colab":{}},"cell_type":"code","source":["# Exercise 3.7 Scratch Code Cell"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BGmnjkgEgYy6","colab_type":"text"},"cell_type":"markdown","source":["# (Some) Data Visualization \n","\n","---\n","\n","Data Visualization is a very broad topic and we will not go into much depth; there is more than enough content to justify semester long courses on the topic. \n","\n","However, `pandas` does provide some very easy to use functionality to produce basic plots and it is a great tool for exploring your data. Visualizing your data can help you spot trends and errors very quickly, which is why it is a useful tool for data wrangling.\n","\n","An example of an error you may find from plotting is extreme outliers in the data set. Although additional research would be needed to confirm these errors, plotting your data is a great place to start looking"]},{"metadata":{"id":"gym4DIsiiZ3U","colab_type":"text"},"cell_type":"markdown","source":["## The `DataFrame` `plot` method\n","\n","The `plot` method that can be called by any `pandas` `DataFrame` is quite versatile and can make the following kinds of plots:\n","\n","| Kind |Description|\n","|:----------|-----------|\n","| `line`| line plot (default) |\n","| `bar` | vertical bar plot  |\n","| `barh` | horizontal bar plot |\n","| `hist` | histogram |\n","| `box` | boxplot  |\n","| `kde` | Kernel Density Estimation plot |\n","| `area` | area plot |\n","| `pie` | pie plot |\n","| `scatter` | scatter plot |\n","| `hexbin` | hexbin plot |\n","\n"," Each kind of plot will require different preparations based on what you would like to show, but we will walk through a basic example of plotting a bar chart of the total spending by doctor specialties. \n"," \n"," To do this we will load a new data set from the file: 'spending_by_specialty.csv' with the following 2 columns\n","\n","| Column |Description|\n","|:----------|-----------|\n","| `Specialty`| The specialty of the doctors who prescribed the medicine (index). |\n","| `total_spending` | The total spending of all the doctors with the specialty. |\n","\n","This data set was built from the data set we have been working with, and we will cover how it was done in the coming Chapter 4: *Data Preparation and Cleaning*\n","\n","If interested, you can learn more about the `plot()` method at the [pandas Documentation website](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html#pandas.DataFrame.plot)\n"]},{"metadata":{"id":"Q0epVuIRobM-","colab_type":"code","colab":{}},"cell_type":"code","source":["spending_by_specialty_df = pd.read_csv('data/spending_by_specialty.csv', \n","                                   index_col='specialty')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QxUIBo2nrLS4","colab_type":"text"},"cell_type":"markdown","source":["## The `DataFrame` `plot` method Bar Graph Example\n","\n","Now that we have the `DataFrame` with the total spending by each specialty we can start building our bar chart. To begin we must let `pyplot` know we are starting a new figure, to do this we use the following syntax:\n","\n","```\n","plt.figure()\n","```\n","\n","Since there are 75 specialties in total we should focus our attention on the top 20 spenders so that the plot is not too crowded. To do this we can sort the `DataFrame` by the `Spending` column in descending order and  chain the `head`method on the returned `DataFrame` with `n=20`. Finally, we can call the `plot()` method with the `kind` parameter set to 'bar'\n","\n","```python\n","spending_by_specialty_df.sort_values('spending').head(n=20).plot(kind='bar')\n","```\n","\n","In Jupyter our plot will automatically show, but if you are working in a Python console then you need to tell `pyplot` to show the figure you have built using the `show()` method of `plt`.\n","\n","```python\n","plt.show()\n","```"]},{"metadata":{"id":"eW2-n5kP0Kzp","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.figure()\n","spending_by_specialty_df.sort_values('spending', ascending=False).head(n=20).plot(kind='bar')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0LJYevev1Ppg","colab_type":"text"},"cell_type":"markdown","source":["### Plot Examples\n","\n","You can make some really neat plots using `pandas` along with `matplotlib`\n","![](images/matplotlib.png)"]},{"metadata":{"id":"rDqp54zSkBIM","colab_type":"text"},"cell_type":"markdown","source":["# Summary\n","\n","---\n","\n","**Summarizing Your Data**\n","\n","* Important Attributes:\n","\n","| Attribute |Description|\n","|:----------|-----------|\n","| `shape`| Return a tuple representing the dimensionality of the DataFrame. |\n","| `size` | Return an int representing the number of elements in this object.  |\n","| `dtypes` | Return the dtypes in the DataFrame. |\n","\n","* Important Methods:\n","\n","| Method|Description|\n","|:----------|-----------|\n","| `head()`| Return the first n rows. |\n","| `tail()` | Return the last n rows. |\n","| `min()`, `max()` | Computes the numeric (for numeric value) or alphanumeric (for object values) row-wise min, max in a Series or DataFrame|\n","| `sum()`, `mean()`, `std()`, `var()`   |  Computes the row-wise sum, mean, standard deviation and variance in a `Series` or DataFrame|\n","| `count()` |  returns the number of non-NaN values in the in a `Series` or `DataFrame` |\n","| `value_counts()` |  returns the frequency for each value in the `Series` |\n","| `describe()` | Computes row-wise statistics |\n","\n","\n","**Arithmetic and Data Alignment**\n","\n","* When executing an arithmetic operation between `Series` or `DataFrames`, the object will first be extended and then aligned by their indices and then the arithmetic will be applied in a pairwise fashion, this is called *vectorized arithmetic between `Series` or `DataFrames`*\n","\n","![](images/alignment_arithmetic_col.png)\n","\n","* When executing an arithmetic operation between constants and `Series` or `Dataframes`, the constant will be extended to a new `Series` or `DataFrame` to align with the first `Series` or `Dataframe` and then the arithmetic will be applied in a pairwise fashion, this is referred to as *broadcasting*\n","\n","![](images/alignment.png)\n","\n","**Subsetting**\n","\n","* Comparison operators are ideal for querying and subsetting the `DataFrame` \n","* We can subset a `Series` using another equally sized list (or a `Series`) of `Boolean`s\n","\n","![](images.filter_dataframe.png)\n","\n","**Sorting**\n","\n","* To sort a `DataFrame` by the values in one of its columns we use the [`pandas` `DataFrame` `sort_values()` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html)\n","\n","**Data Visualization**\n","\n","* The `plot` method that may be called by any `pandas` `DataFrame` is quite versatile and can make the following kinds of plots:\n","\n","| Kind |Description|\n","|:----------|-----------|\n","| `line`| line plot (default) |\n","| `bar` | vertical bar plot  |\n","| `barh` | horizontal bar plot |\n","| `hist` | histogram |\n","| `box` | boxplot  |\n","| `kde` | Kernel Density Estimation plot |\n","| `area` | area plot |\n","| `pie` | pie plot |\n","| `scatter` | scatter plot |\n","| `hexbin` | hexbin plot |\n","\n","  * To learn more about the `plot` method please see the [pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html#pandas.DataFrame.plot)\n","\n"]}]}